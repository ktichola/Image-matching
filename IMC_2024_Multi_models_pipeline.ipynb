{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 71885,
          "databundleVersionId": 8143495,
          "sourceType": "competition"
        },
        {
          "sourceId": 3414836,
          "sourceType": "datasetVersion",
          "datasetId": 2058261
        },
        {
          "sourceId": 5373920,
          "sourceType": "datasetVersion",
          "datasetId": 3117886
        },
        {
          "sourceId": 7884485,
          "sourceType": "datasetVersion",
          "datasetId": 4628051
        },
        {
          "sourceId": 170475544,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 170565695,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 174129945,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 3736,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 2663
        },
        {
          "sourceId": 3840,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 2742
        },
        {
          "sourceId": 3846,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 2747
        },
        {
          "sourceId": 4534,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3326
        },
        {
          "sourceId": 17191,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 14317
        },
        {
          "sourceId": 17555,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 14611
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "IMC-2024-Multi models pipeline ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktichola/Image-matching/blob/main/IMC_2024_Multi_models_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'image-matching-challenge-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F71885%2F8143495%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D13551573aa4a4cb329e5728667ea9e56c5def6e03687e4c3b68598ef3a0f632e21ac68fced5e32e8c63a72d68610e6434acce574e0492e883ef6640093f128cc9d448f110929dfd35aee695f7b53b920531b04e4b18addf3337e8eeee94528b2bd883067129c16738b4b29b1991ca30373afba7ff534a7a5b23a9789344b5528f0e7cfecb8c3697754873aadcf93ecdac1eca78d1860f5111a3f80c5af8c40721de75ff77c6e4ddee01bf756f8ba3a55306578712c8c340ab4bd60e9cd9810e46c9f1cfb9466f769a37d5eceb98098ef997718082f9561ebbfc514b02cc07ac750b1a467f58e798f998b46fbc0e05d7069d2c9c8389307e558d472a14d94dbe2,super-glue-pretrained-network:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2058261%2F3414836%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D34f33d0d20f564331a0d247d6fc3e595b5080ab8ba69a7a66abebb501bba278c55adf01c9463e23552ba5d5487268e80a23224ffee01786fb91437396a3b568706e995db54e75620ac14f8207bdccc8d3c8e6b90a957d6dcb2dc338bc4837c67dbaebcf19606b5aa2bdc350953a0effccb830ed0df65b4f9dc8f3d02866c8bc794329a53973890b4c859096d87e91fa4c13fc32b2fc7361741db3cb24f49f45e55a0e15b0f76462108366cdb94edcb7b16b90690b86ed4dc2c0a4730f409e9bd63c97db3128c365e3c9fb080dd475df55b937eccf4cf88e0d398a9b98a3f77c1cab6f54da33b4491f9cfa23a955df1b218165363f244e4fc288ab849482d3e3c,kornia-local-feature-weights:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3117886%2F5373920%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8fa74194a7ac4c0c2893e4f33940f080c9d7e870036c07f3abdb7d2b5256d17aed716071070a1b655e8900fc1fec3d75efb6acc5fbf5260af24c3a6054b1743fbc2509f028f61c89627b50d2f1fb12cffa009c90a647b54bd000aa5b9d8db17282bb76899b11d7d45737851dc197f4f9b37560d78afcfe8d09bcdb1ea03bcdaf9d9406534e5916de7090f0c46562738e19e670f7060c9f7f5d90fa9116e6452ddce9ae88fec975a32ac74fae9f5c4af30947c432abf5b9a3639a2b528da6d8b142582a9b98819d97f30a266f4cd9014a8e2a24e148d853b3dee09b7279e9a72da1c6678cd1e3fdbc59fe9dd95d3873ecd39ecc9167e0b5b7585791f5bd030ad9,imc2024-packages-lightglue-rerun-kornia:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4628051%2F7884485%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9fce8669b581cd2efa39d6e216aa7329c9b42e74b9c0f7cd5ce2c7c283d17ab7398bd0c69aecffbd3d5c85ea7d43c3cd67d86536f4a814ece4f4993aff2c6e4a89d72093f5ae8226b30e82fc70968aaa284f06ffd396a4b0beeeee55cf08a30ac1666d8265bee4632d8013905c8d3b09ddaed6a3bed3d20e73798df72d61a90c375f6e5d3e8ba9abe5929df700857f1cab430863d1f71c9e3c6523a27b6141857d6b2a91446bf3c30f768ff776f01d5e253515adacb541be7d280b5627637c2c43faef4b034025a6c3289ecfa4a5b92d894c772f10f29472e15ad1fd27151ec65107e7d915d0f512e0618f1dcc31a1ba0ce373f70b4da2fdce1ddbae75c8f9f1,tf-efficientnet/pytorch/tf-efficientnet-b7/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F2663%2F3736%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dadc1818f4d95ff1f9d92d63411ba60842be4dfba39f201faba40c3af7c82d4c578b293890a42e3d3d21918031c6184fb8af389fd0a18ba0e66583a433c7c3d39ff218a9bfd2721a58c8912d251c9e5f3529c882f08c3e1473956955857ecd28a72f8b4a45f3324f445a0a8e93884bca09ba1455a85ab98c2afa5cb716765b79b15a5079ebc49abb2bb775cc1ac83aace2a68c2c97e58d919c8e75518dd8a2474016d5cadf29dfb6a0d964f903e1bf45538449d843dcebaead99aa0b9af764424a4f03308b7534b5d918e60c908165204b8085351f04dabc8bf42e8e2c62b8cf4da25edcb5826aa232f1623f1ee25f3f5c974c085cbbfce7f6dee79a5e5372905,loftr/pytorch/outdoor/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F2742%2F3840%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1347db2e685149c26e233f4bff4a042eb96f09c5403b0bf4da4b29097a89a35ff0af47fd49fdb34d2cb94a2534e2a2e8c4583499d0d5b97069831a3c6a2d0ac06b9ea5c51a508a8425b0e393fc67ee66b313b8c501061f99afeed071b86efb1278a4a240acb20232dbe47c3dd55fdc138dfea6322a311b1bcca84a35d653e1c10dcd48f3fa35f540eaa3acf7b6a27c8e36d329f2f205143ebddc847f336485b558e131b171fb2608fee53d265a301c16b7d134f2e15fecfa84d9d27bc517bb69d67951f39ae5af70c56293d4d4d47ee01b4762a6a0793b8d0133435093a729c8b952e872fd89599a0e7bfb55e57b5f94efcce5ac0e8cd2fab84104a0792c19d6,disk/pytorch/depth-supervision/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F2747%2F3846%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D31f4f756e9eda9b02f98c00a0b22ffe7ba01fe2e8e6291463f076e75811802adda3801e9130bfeabb9ce367fcf3b9665c3f6fb91e5b3f9a6c17b26a43ef57c9b8d8908c0e9bd617257fb5ca8ddf6cf364a0a4b907219476db1a2192bbbf7e5fd206e32e16ef17e9cbb6abec09cf7299119603dfd22605155f4ebb0ca39ad949cfeba33903c49d771e2fd54a772f5f472dc88688f43d23258af37c799a7470367ae3d1cd591f2a4139779b0758020c74fe29b38c222a9b442f3308f6d3285bf79cd0c64593f923bf02fa5c56721f4f077d655bea3feffdad17a2eb8650498c95e894bbe0d823a67cd283095e11a3932ec4f6ccb5b52ab4ccfd44eaa4f5f0f6f8d,dinov2/pytorch/base/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3326%2F4534%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D859c071a40204b6309293d00a24fa0c4db24e4651ad04b0695dc134d107bf557d2290f5c2776571fc64e9e63144ebd6ebb6d154e5fc5b262cc4afd6e23b826c7b7bffe8829e5a06eb7fa316c22bfedb2175e9c845d62fdaf79301f634591477bca43a433499e15ff109c29115cb5bc365f3f350989f83f218752ab97523a9473439469af57ab84d1e2a2b233fddf88f4387e370501607718c19089d29951f56b3d76df765d157562c49ec122f222109e5188aa3d2d3c87ab7859228e0b0cb8abc9b2f1404833a23ce284336d50cd4c5d41661e711111b85bbf5bd6324a9798e1adbff0d8b3a20a1dd067b0d188985c9cfc31f45601647c753a28e4aff72ea808,lightglue/pytorch/aliked/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F14317%2F17191%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070449Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ec80dec866009b68e4a56c9da14a6622d84c09b866d64f9fdb811117119c59d7331f02ce82ba3cb7a6159e961d89ec58e1aa58c63ceff865913e76539b0cea9fa803a902d9b4be21e66c90d6741faa539d99b03170f1c15ad3e8c6af57471021479eb213a6c024035a0225bb31ff327800bd87993fb9eac18236c9c12a8bc16bb43a618eca34f05612102ae624e675a474eaaa670708faff9ae721f22e274de6cde7d74f4eb537ac328d1b68fab58f7998a3013f4592949d05c9ea4341f351ac6ec160f605f1aae2a73deb21df8558b5b503c1b20dd3ea2a793fa9ddb0350ea6cd9f25dc5bd7333ecef1f1d8b9181bc5e63748b3b8f3ffcf40e8322d571dced,aliked/pytorch/aliked-n16/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F14611%2F17555%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T070449Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D54719dccd209bc752f5976bf177199395b413b44bb3f57cff99a46c0473ebc147e0cc6c56618b9e7aa20083f4611c5169833f408fb8940b2d66f6effbb17dcf6d3589d342bd8b90e96009f973b3c0bc080066bac571aa5c217fd3b3b8c2a6be8a3b5fec6b1c42e1c21469e2f4f50df996f3851d9190fce0fcbd6b3fced339a43478cd5f204d33d7df5e12e896d043f8f8badccd98d4689945c1faa1f87a4ae0a593078e416f26826490ae17d6d3a856c19ad14369ad256fc56ba7ad445adeca3f90a17c69cf8530edc4bbea66349aba59090881288d1e275d9a0e5347f9ebf0f5690c2c129811bddbef288a98cb00a15e5e5135950d5e9dd092855a760303db3'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "D30hWosb6yL4"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "pYLg8qo46yL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --no-deps /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\n",
        "!python -m pip install --no-deps /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!python -m pip install --no-index --find-links=/kaggle/input/dependencies-imc/transformers/ transformers > /dev/null\n",
        "!python -m pip install  --no-deps /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n",
        "\n",
        "# dkm\n",
        "!python -m pip install --no-index --find-links=/kaggle/input/dkm-dependencies/packages einops > /dev/null"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:47:05.273558Z",
          "iopub.execute_input": "2024-05-14T06:47:05.273894Z",
          "iopub.status.idle": "2024-05-14T06:48:37.294051Z",
          "shell.execute_reply.started": "2024-05-14T06:47:05.273867Z",
          "shell.execute_reply": "2024-05-14T06:48:37.292664Z"
        },
        "trusted": true,
        "id": "w_LZPf296yL7",
        "outputId": "79486f9b-4b51-4aa4-a982-2c805cf83a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Processing /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\nInstalling collected packages: pycolmap\nSuccessfully installed pycolmap-0.4.0\nProcessing /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: safetensors\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.4.3\n    Uninstalling safetensors-0.4.3:\n      Successfully uninstalled safetensors-0.4.3\nSuccessfully installed safetensors-0.4.1\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nInstalling collected packages: lightglue\nSuccessfully installed lightglue-0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lightglue models\n",
        "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
        "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
        "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
        "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
        "!cp /kaggle/input/pytorch-lightglue-models/* /root/.cache/torch/hub/checkpoints/\n",
        "\n",
        "# dkm model\n",
        "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
        "!cp /kaggle/input/dkm-dependencies/DKMv3_outdoor.pth /root/.cache/torch/hub/checkpoints/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:37.296309Z",
          "iopub.execute_input": "2024-05-14T06:48:37.296632Z",
          "iopub.status.idle": "2024-05-14T06:48:49.049609Z",
          "shell.execute_reply.started": "2024-05-14T06:48:37.296599Z",
          "shell.execute_reply": "2024-05-14T06:48:49.048528Z"
        },
        "trusted": true,
        "id": "3c_MEkZI6yL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:49.05519Z",
          "iopub.execute_input": "2024-05-14T06:48:49.055731Z",
          "iopub.status.idle": "2024-05-14T06:48:49.062606Z",
          "shell.execute_reply.started": "2024-05-14T06:48:49.055674Z",
          "shell.execute_reply": "2024-05-14T06:48:49.061761Z"
        },
        "trusted": true,
        "id": "zxn_KLfZ6yL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General utilities\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "from fastprogress import progress_bar\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from IPython.display import clear_output\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "\n",
        "# CV/ML\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import kornia as K\n",
        "import kornia.feature as KF\n",
        "from PIL import Image\n",
        "import timm\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform\n",
        "\n",
        "import torchvision\n",
        "\n",
        "# 3D reconstruction\n",
        "import pycolmap\n",
        "\n",
        "import glob\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# dkm\n",
        "import sys\n",
        "sys.path.append('/kaggle/input/dkm-dependencies/DKM/')\n",
        "from dkm.utils.utils import tensor_to_pil, get_tuple_transform_ops\n",
        "from dkm import DKMv3_outdoor\n",
        "\n",
        "# LoFTR\n",
        "from kornia.feature import LoFTR"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:49.064001Z",
          "iopub.execute_input": "2024-05-14T06:48:49.064293Z",
          "iopub.status.idle": "2024-05-14T06:48:57.420516Z",
          "shell.execute_reply.started": "2024-05-14T06:48:49.064268Z",
          "shell.execute_reply": "2024-05-14T06:48:57.419557Z"
        },
        "trusted": true,
        "id": "7818-BKv6yL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightglue import match_pair\n",
        "from lightglue import ALIKED, SuperPoint, DoGHardNet, LightGlue\n",
        "from lightglue.utils import load_image, rbd\n",
        "from kornia.feature import LoFTR"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.421543Z",
          "iopub.execute_input": "2024-05-14T06:48:57.422157Z",
          "iopub.status.idle": "2024-05-14T06:48:57.430483Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.422129Z",
          "shell.execute_reply": "2024-05-14T06:48:57.4295Z"
        },
        "trusted": true,
        "id": "P6k1MNrr6yL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Kornia version', K.__version__)\n",
        "print('Pycolmap version', pycolmap.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.431512Z",
          "iopub.execute_input": "2024-05-14T06:48:57.431821Z",
          "iopub.status.idle": "2024-05-14T06:48:57.444088Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.431796Z",
          "shell.execute_reply": "2024-05-14T06:48:57.443183Z"
        },
        "trusted": true,
        "id": "w5qFpYng6yL9",
        "outputId": "314ad337-da36-4751-cb41-f978ec8bfd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Kornia version 0.7.2\nPycolmap version 0.4.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "0Kg2ujeB6yL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CONFIG:\n",
        "    use_aliked_lightglue = True\n",
        "    use_doghardnet_lightglue = False\n",
        "    use_superpoint_lightglue = False\n",
        "    use_loftr = False\n",
        "    use_dkm = False\n",
        "    use_superglue = False\n",
        "\n",
        "    params_aliked_lightglue = {\n",
        "        \"num_features\" : 8192,\n",
        "        \"detection_threshold\" : 0.001,\n",
        "        \"min_matches\" : 15,\n",
        "        \"resize_to\" : 1024,\n",
        "    }\n",
        "\n",
        "    params_doghardnet_lightglue = {\n",
        "        \"num_features\" : 8192,\n",
        "        \"detection_threshold\" : 0.001,\n",
        "        \"min_matches\" : 15,\n",
        "        \"resize_to\" : 1024,\n",
        "    }\n",
        "\n",
        "    params_superpoint_lightglue = {\n",
        "        \"num_features\" : 4096,\n",
        "        \"detection_threshold\" : 0.005,\n",
        "        \"min_matches\" : 15,\n",
        "        \"resize_to\" : 1024,\n",
        "    }\n",
        "\n",
        "    params_loftr = {\n",
        "        \"resize_small_edge_to\" : 750,\n",
        "        \"min_matches\" : 15,\n",
        "    }\n",
        "\n",
        "    params_dkm = {\n",
        "        \"num_features\" : 2048,\n",
        "        \"detection_threshold\" : 0.4,\n",
        "        \"min_matches\" : 15,\n",
        "        \"resize_to\" : (540, 720),\n",
        "    }\n",
        "\n",
        "    params_sg = {\n",
        "        \"sg_config\" :\n",
        "        {\n",
        "            \"superpoint\": {\n",
        "                \"nms_radius\": 4,\n",
        "                \"keypoint_threshold\": 0.0001,\n",
        "                \"max_keypoints\": 4096\n",
        "            },\n",
        "            \"superglue\": {\n",
        "                \"weights\": \"outdoor\",\n",
        "                \"sinkhorn_iterations\": 10,\n",
        "                \"match_threshold\": 0.2,\n",
        "            },\n",
        "        },\n",
        "        \"resize_to\": 1240,\n",
        "        \"min_matches\": 15,\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.445229Z",
          "iopub.execute_input": "2024-05-14T06:48:57.445509Z",
          "iopub.status.idle": "2024-05-14T06:48:57.454426Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.445486Z",
          "shell.execute_reply": "2024-05-14T06:48:57.453566Z"
        },
        "trusted": true,
        "id": "rQECOTeA6yL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.455473Z",
          "iopub.execute_input": "2024-05-14T06:48:57.455776Z",
          "iopub.status.idle": "2024-05-14T06:48:57.465958Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.455753Z",
          "shell.execute_reply": "2024-05-14T06:48:57.465202Z"
        },
        "trusted": true,
        "id": "Fc5Yvdyy6yL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COLMAP utilities"
      ],
      "metadata": {
        "id": "78IxdDHl6yL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to manipulate a colmap database.\n",
        "# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
        "\n",
        "# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n",
        "# All rights reserved.\n",
        "#\n",
        "# Redistribution and use in source and binary forms, with or without\n",
        "# modification, are permitted provided that the following conditions are met:\n",
        "#\n",
        "#     * Redistributions of source code must retain the above copyright\n",
        "#       notice, this list of conditions and the following disclaimer.\n",
        "#\n",
        "#     * Redistributions in binary form must reproduce the above copyright\n",
        "#       notice, this list of conditions and the following disclaimer in the\n",
        "#       documentation and/or other materials provided with the distribution.\n",
        "#\n",
        "#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n",
        "#       its contributors may be used to endorse or promote products derived\n",
        "#       from this software without specific prior written permission.\n",
        "#\n",
        "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
        "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n",
        "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
        "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
        "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
        "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
        "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
        "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
        "# POSSIBILITY OF SUCH DAMAGE.\n",
        "#\n",
        "# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n",
        "\n",
        "# This script is based on an original implementation by True Price.\n",
        "\n",
        "import sys\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "IS_PYTHON3 = sys.version_info[0] >= 3\n",
        "\n",
        "MAX_IMAGE_ID = 2**31 - 1\n",
        "\n",
        "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
        "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "    model INTEGER NOT NULL,\n",
        "    width INTEGER NOT NULL,\n",
        "    height INTEGER NOT NULL,\n",
        "    params BLOB,\n",
        "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
        "\n",
        "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
        "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
        "    rows INTEGER NOT NULL,\n",
        "    cols INTEGER NOT NULL,\n",
        "    data BLOB,\n",
        "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
        "\n",
        "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
        "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "    name TEXT NOT NULL UNIQUE,\n",
        "    camera_id INTEGER NOT NULL,\n",
        "    prior_qw REAL,\n",
        "    prior_qx REAL,\n",
        "    prior_qy REAL,\n",
        "    prior_qz REAL,\n",
        "    prior_tx REAL,\n",
        "    prior_ty REAL,\n",
        "    prior_tz REAL,\n",
        "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
        "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
        "\"\"\".format(MAX_IMAGE_ID)\n",
        "\n",
        "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
        "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "    rows INTEGER NOT NULL,\n",
        "    cols INTEGER NOT NULL,\n",
        "    data BLOB,\n",
        "    config INTEGER NOT NULL,\n",
        "    F BLOB,\n",
        "    E BLOB,\n",
        "    H BLOB)\n",
        "\"\"\"\n",
        "\n",
        "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
        "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
        "    rows INTEGER NOT NULL,\n",
        "    cols INTEGER NOT NULL,\n",
        "    data BLOB,\n",
        "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
        "\"\"\"\n",
        "\n",
        "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
        "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "    rows INTEGER NOT NULL,\n",
        "    cols INTEGER NOT NULL,\n",
        "    data BLOB)\"\"\"\n",
        "\n",
        "CREATE_NAME_INDEX = \\\n",
        "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
        "\n",
        "CREATE_ALL = \"; \".join([\n",
        "    CREATE_CAMERAS_TABLE,\n",
        "    CREATE_IMAGES_TABLE,\n",
        "    CREATE_KEYPOINTS_TABLE,\n",
        "    CREATE_DESCRIPTORS_TABLE,\n",
        "    CREATE_MATCHES_TABLE,\n",
        "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
        "    CREATE_NAME_INDEX\n",
        "])\n",
        "\n",
        "\n",
        "def image_ids_to_pair_id(image_id1, image_id2):\n",
        "    if image_id1 > image_id2:\n",
        "        image_id1, image_id2 = image_id2, image_id1\n",
        "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
        "\n",
        "\n",
        "def pair_id_to_image_ids(pair_id):\n",
        "    image_id2 = pair_id % MAX_IMAGE_ID\n",
        "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
        "    return image_id1, image_id2\n",
        "\n",
        "\n",
        "def array_to_blob(array):\n",
        "    if IS_PYTHON3:\n",
        "        return array.tostring()\n",
        "    else:\n",
        "        return np.getbuffer(array)\n",
        "\n",
        "\n",
        "def blob_to_array(blob, dtype, shape=(-1,)):\n",
        "    if IS_PYTHON3:\n",
        "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
        "    else:\n",
        "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
        "\n",
        "\n",
        "class COLMAPDatabase(sqlite3.Connection):\n",
        "\n",
        "    @staticmethod\n",
        "    def connect(database_path):\n",
        "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
        "\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
        "        self.create_cameras_table = \\\n",
        "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
        "        self.create_descriptors_table = \\\n",
        "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
        "        self.create_images_table = \\\n",
        "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
        "        self.create_two_view_geometries_table = \\\n",
        "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
        "        self.create_keypoints_table = \\\n",
        "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
        "        self.create_matches_table = \\\n",
        "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
        "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
        "\n",
        "    def add_camera(self, model, width, height, params,\n",
        "                   prior_focal_length=False, camera_id=None):\n",
        "        params = np.asarray(params, np.float64)\n",
        "        cursor = self.execute(\n",
        "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "            (camera_id, model, width, height, array_to_blob(params),\n",
        "             prior_focal_length))\n",
        "        return cursor.lastrowid\n",
        "\n",
        "    def add_image(self, name, camera_id,\n",
        "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
        "        cursor = self.execute(\n",
        "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
        "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
        "        return cursor.lastrowid\n",
        "\n",
        "    def add_keypoints(self, image_id, keypoints):\n",
        "        assert(len(keypoints.shape) == 2)\n",
        "        assert(keypoints.shape[1] in [2, 4, 6])\n",
        "\n",
        "        keypoints = np.asarray(keypoints, np.float32)\n",
        "        self.execute(\n",
        "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
        "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
        "\n",
        "    def add_descriptors(self, image_id, descriptors):\n",
        "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
        "        self.execute(\n",
        "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
        "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
        "\n",
        "    def add_matches(self, image_id1, image_id2, matches):\n",
        "        assert(len(matches.shape) == 2)\n",
        "        assert(matches.shape[1] == 2)\n",
        "\n",
        "        if image_id1 > image_id2:\n",
        "            matches = matches[:,::-1]\n",
        "\n",
        "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
        "        matches = np.asarray(matches, np.uint32)\n",
        "        self.execute(\n",
        "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
        "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
        "\n",
        "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
        "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
        "        assert(len(matches.shape) == 2)\n",
        "        assert(matches.shape[1] == 2)\n",
        "\n",
        "        if image_id1 > image_id2:\n",
        "            matches = matches[:,::-1]\n",
        "\n",
        "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
        "        matches = np.asarray(matches, np.uint32)\n",
        "        F = np.asarray(F, dtype=np.float64)\n",
        "        E = np.asarray(E, dtype=np.float64)\n",
        "        H = np.asarray(H, dtype=np.float64)\n",
        "        self.execute(\n",
        "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
        "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.469156Z",
          "iopub.execute_input": "2024-05-14T06:48:57.469435Z",
          "iopub.status.idle": "2024-05-14T06:48:57.498656Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.469402Z",
          "shell.execute_reply": "2024-05-14T06:48:57.497457Z"
        },
        "trusted": true,
        "id": "RhYESVUS6yL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# h5 to colmap db"
      ],
      "metadata": {
        "id": "hfwSX9kL6yL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to interface DISK with Colmap.\n",
        "# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n",
        "\n",
        "#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n",
        "#\n",
        "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#   you may not use this file except in compliance with the License.\n",
        "#   You may obtain a copy of the License at\n",
        "#\n",
        "#       http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#   Unless required by applicable law or agreed to in writing, software\n",
        "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#   See the License for the specific language governing permissions and\n",
        "#   limitations under the License.\n",
        "\n",
        "import os, argparse, h5py, warnings\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ExifTags\n",
        "\n",
        "\n",
        "def get_focal(image_path, err_on_default=False):\n",
        "    image         = Image.open(image_path)\n",
        "    max_size      = max(image.size)\n",
        "\n",
        "    exif = image.getexif()\n",
        "    focal = None\n",
        "    if exif is not None:\n",
        "        focal_35mm = None\n",
        "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
        "        for tag, value in exif.items():\n",
        "            focal_35mm = None\n",
        "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
        "                focal_35mm = float(value)\n",
        "                break\n",
        "\n",
        "        if focal_35mm is not None:\n",
        "            focal = focal_35mm / 35. * max_size\n",
        "\n",
        "    if focal is None:\n",
        "        if err_on_default:\n",
        "            raise RuntimeError(\"Failed to find focal length\")\n",
        "\n",
        "        # failed to find it in exif, use prior\n",
        "        FOCAL_PRIOR = 1.2\n",
        "        focal = FOCAL_PRIOR * max_size\n",
        "\n",
        "    return focal\n",
        "\n",
        "def create_camera(db, image_path, camera_model):\n",
        "    image         = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "\n",
        "    focal = get_focal(image_path)\n",
        "\n",
        "    if camera_model == 'simple-pinhole':\n",
        "        model = 0 # simple pinhole\n",
        "        param_arr = np.array([focal, width / 2, height / 2])\n",
        "    if camera_model == 'pinhole':\n",
        "        model = 1 # pinhole\n",
        "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
        "    elif camera_model == 'simple-radial':\n",
        "        model = 2 # simple radial\n",
        "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
        "    elif camera_model == 'opencv':\n",
        "        model = 4 # opencv\n",
        "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
        "\n",
        "    return db.add_camera(model, width, height, param_arr)\n",
        "\n",
        "\n",
        "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
        "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
        "\n",
        "    camera_id = None\n",
        "    fname_to_id = {}\n",
        "    for filename in tqdm(list(keypoint_f.keys())):\n",
        "        keypoints = keypoint_f[filename][()]\n",
        "\n",
        "        fname_with_ext = filename# + img_ext\n",
        "        path = os.path.join(image_path, fname_with_ext)\n",
        "        if not os.path.isfile(path):\n",
        "            raise IOError(f'Invalid image path {path}')\n",
        "\n",
        "        if camera_id is None or not single_camera:\n",
        "            camera_id = create_camera(db, path, camera_model)\n",
        "        image_id = db.add_image(fname_with_ext, camera_id)\n",
        "        fname_to_id[filename] = image_id\n",
        "\n",
        "        db.add_keypoints(image_id, keypoints)\n",
        "\n",
        "    return fname_to_id\n",
        "\n",
        "def add_matches(db, h5_path, fname_to_id):\n",
        "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
        "\n",
        "    added = set()\n",
        "    n_keys = len(match_file.keys())\n",
        "    n_total = (n_keys * (n_keys - 1)) // 2\n",
        "\n",
        "    with tqdm(total=n_total) as pbar:\n",
        "        for key_1 in match_file.keys():\n",
        "            group = match_file[key_1]\n",
        "            for key_2 in group.keys():\n",
        "                id_1 = fname_to_id[key_1]\n",
        "                id_2 = fname_to_id[key_2]\n",
        "\n",
        "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
        "                if pair_id in added:\n",
        "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
        "                    continue\n",
        "\n",
        "                matches = group[key_2][()]\n",
        "                db.add_matches(id_1, id_2, matches)\n",
        "\n",
        "                added.add(pair_id)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "def import_into_colmap(img_dir,\n",
        "                       feature_dir ='.featureout',\n",
        "                       database_path = 'colmap.db',\n",
        "                       img_ext='.jpg'):\n",
        "    db = COLMAPDatabase.connect(database_path)\n",
        "    db.create_tables()\n",
        "    single_camera = False\n",
        "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, 'simple-radial', single_camera)\n",
        "    add_matches(\n",
        "        db,\n",
        "        feature_dir,\n",
        "        fname_to_id,\n",
        "    )\n",
        "\n",
        "    db.commit()\n",
        "    return"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.499951Z",
          "iopub.execute_input": "2024-05-14T06:48:57.500211Z",
          "iopub.status.idle": "2024-05-14T06:48:57.521071Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.500189Z",
          "shell.execute_reply": "2024-05-14T06:48:57.52019Z"
        },
        "trusted": true,
        "id": "r6i1VR1G6yL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Pairs"
      ],
      "metadata": {
        "id": "dc3zWFn66yL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use ViT global descriptor to get matching shortlists.\n",
        "def get_global_desc(fnames, model,\n",
        "                    device =  torch.device('cpu')):\n",
        "    model = model.eval()\n",
        "    model= model.to(device)\n",
        "    config = resolve_data_config({}, model=model)\n",
        "    transform = create_transform(**config)\n",
        "    global_descs_convnext=[]\n",
        "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
        "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
        "        img = Image.open(img_fname_full).convert('RGB')\n",
        "        timg = transform(img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
        "            #print (desc.shape)\n",
        "            desc = desc.view(1, -1)\n",
        "            desc_norm = F.normalize(desc, dim=1, p=2)\n",
        "        #print (desc_norm)\n",
        "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
        "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
        "    return global_descs_all.to(torch.float32)\n",
        "\n",
        "def convert_1d_to_2d(idx, num_images):\n",
        "    idx1 = idx // num_images\n",
        "    idx2 = idx % num_images\n",
        "    return (idx1, idx2)\n",
        "\n",
        "def get_pairs_from_distancematrix(mat):\n",
        "    pairs = [ convert_1d_to_2d(idx, mat.shape[0]) for idx in np.argsort(mat.flatten())]\n",
        "    pairs = [ pair for pair in pairs if pair[0] < pair[1] ]\n",
        "    return pairs\n",
        "\n",
        "def get_img_pairs_exhaustive(img_fnames, model, device):\n",
        "    #index_pairs = []\n",
        "    #for i in range(len(img_fnames)):\n",
        "    #    for j in range(i+1, len(img_fnames)):\n",
        "    #        index_pairs.append((i,j))\n",
        "    #return index_pairs\n",
        "    descs = get_global_desc(img_fnames, model, device=device)\n",
        "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
        "    matching_list = get_pairs_from_distancematrix(dm)\n",
        "    return matching_list\n",
        "\n",
        "\n",
        "def get_image_pairs_shortlist(fnames,\n",
        "                              sim_th = 0.6, # should be strict\n",
        "                              min_pairs = 20,\n",
        "                              exhaustive_if_less = 20,\n",
        "                              device=torch.device('cpu')):\n",
        "    num_imgs = len(fnames)\n",
        "\n",
        "    model = timm.create_model('tf_efficientnet_b7',\n",
        "                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
        "    model.eval()\n",
        "    descs = get_global_desc(fnames, model, device=device)\n",
        "\n",
        "    if num_imgs <= exhaustive_if_less:\n",
        "        return get_img_pairs_exhaustive(fnames, model, device)\n",
        "\n",
        "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
        "    # removing half\n",
        "    mask = dm <= sim_th\n",
        "    total = 0\n",
        "    matching_list = []\n",
        "    ar = np.arange(num_imgs)\n",
        "    already_there_set = []\n",
        "    for st_idx in range(num_imgs-1):\n",
        "        mask_idx = mask[st_idx]\n",
        "        to_match = ar[mask_idx]\n",
        "        if len(to_match) < min_pairs:\n",
        "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
        "        for idx in to_match:\n",
        "            if st_idx == idx:\n",
        "                continue\n",
        "            if dm[st_idx, idx] < 1000:\n",
        "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
        "                total+=1\n",
        "    matching_list = sorted(list(set(matching_list)))\n",
        "    return matching_list"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.522084Z",
          "iopub.execute_input": "2024-05-14T06:48:57.522315Z",
          "iopub.status.idle": "2024-05-14T06:48:57.539906Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.522295Z",
          "shell.execute_reply": "2024-05-14T06:48:57.538978Z"
        },
        "trusted": true,
        "id": "iuD4whdO6yL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoints: LightGlue series"
      ],
      "metadata": {
        "id": "spjMkPa66yL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_torch_image(fname, device=torch.device('cpu')):\n",
        "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
        "    return img\n",
        "\n",
        "def detect_common(img_fnames,\n",
        "                  model_name,\n",
        "                  feature_dir = '.featureout',\n",
        "                  num_features = 4096,\n",
        "                  resize_to = 1024,\n",
        "                  detection_threshold = 0.01,\n",
        "                  device=torch.device('cpu')):\n",
        "\n",
        "    dict_model = {\n",
        "        \"aliked\" : ALIKED,\n",
        "        \"superpoint\" : SuperPoint,\n",
        "        \"doghardnet\" : DoGHardNet,\n",
        "    }\n",
        "    extractor_class = dict_model[model_name]\n",
        "\n",
        "    dtype = torch.float32 # ALIKED has issues with float16\n",
        "    extractor = extractor_class(\n",
        "        max_num_keypoints=num_features, detection_threshold=detection_threshold, resize=resize_to\n",
        "    ).eval().to(device, dtype)\n",
        "    if not os.path.isdir(feature_dir):\n",
        "        os.makedirs(feature_dir)\n",
        "    with h5py.File(f'{feature_dir}/keypoints_{model_name}.h5', mode='w') as f_kp, \\\n",
        "         h5py.File(f'{feature_dir}/descriptors_{model_name}.h5', mode='w') as f_desc:\n",
        "        for img_path in tqdm(img_fnames):\n",
        "            img_fname = img_path.split('/')[-1]\n",
        "            key = img_fname\n",
        "            with torch.inference_mode():\n",
        "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
        "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
        "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
        "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
        "                f_kp[key] = kpts\n",
        "                f_desc[key] = descs\n",
        "                print(f\"{model_name} > kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
        "    return\n",
        "\n",
        "def match_with_lightglue_common(img_fnames, model_name,\n",
        "                   index_pairs, file_keypoints,\n",
        "                   feature_dir = '.featureout',\n",
        "                   device=torch.device('cpu'),\n",
        "                   min_matches=15,verbose=True):\n",
        "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
        "                                                \"depth_confidence\": -1,\n",
        "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
        "\n",
        "    cnt_pairs = 0\n",
        "    with h5py.File(f'{feature_dir}/keypoints_{model_name}.h5', mode='r') as f_kp, \\\n",
        "        h5py.File(f'{feature_dir}/descriptors_{model_name}.h5', mode='r') as f_desc, \\\n",
        "        h5py.File(file_keypoints, mode='w') as f_match:\n",
        "        for pair_idx in tqdm(index_pairs):\n",
        "            idx1, idx2 = pair_idx\n",
        "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "\n",
        "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
        "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
        "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
        "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
        "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
        "            with torch.inference_mode():\n",
        "                dists, idxs = lg_matcher(desc1,\n",
        "                                         desc2,\n",
        "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
        "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
        "            if len(idxs)  == 0:\n",
        "                continue\n",
        "            n_matches = len(idxs)\n",
        "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
        "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
        "            group  = f_match.require_group(key1)\n",
        "            if n_matches >= min_matches:\n",
        "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
        "                cnt_pairs+=1\n",
        "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')\n",
        "            else:\n",
        "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
        "    return\n",
        "\n",
        "def detect_lightglue_common(\n",
        "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints,\n",
        "    resize_to=1024,\n",
        "    detection_threshold=0.01,\n",
        "    num_features=4096,\n",
        "    min_matches=15,\n",
        "):\n",
        "    t=time()\n",
        "    detect_common(\n",
        "        img_fnames, model_name, feature_dir,\n",
        "        resize_to=resize_to,\n",
        "        num_features=num_features,\n",
        "        detection_threshold=detection_threshold,\n",
        "        device=device\n",
        "    )\n",
        "    gc.collect()\n",
        "    match_with_lightglue_common(\n",
        "        img_fnames, model_name, index_pairs, file_keypoints,\n",
        "        feature_dir=feature_dir,\n",
        "        min_matches=min_matches,\n",
        "        device=device\n",
        "    )\n",
        "    t=time() -t\n",
        "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
        "    return t"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.54106Z",
          "iopub.execute_input": "2024-05-14T06:48:57.541319Z",
          "iopub.status.idle": "2024-05-14T06:48:57.563772Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.541296Z",
          "shell.execute_reply": "2024-05-14T06:48:57.562887Z"
        },
        "trusted": true,
        "id": "rf1hvdCf6yL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoints: SuperGlue"
      ],
      "metadata": {
        "id": "LxyctJs_6yL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
        "from models.matching import Matching\n",
        "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
        "                          estimate_pose, make_matching_plot,\n",
        "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
        "                          process_resize, frame2tensor,\n",
        "                          rotate_intrinsics, rotate_pose_inplane,\n",
        "                          scale_intrinsics)\n",
        "\n",
        "from torch.nn import functional as torchF  # For resizing tensor\n",
        "\n",
        "def sg_imread(path):\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    return image\n",
        "\n",
        "# Preprocess\n",
        "def sg_read_image(image, device, resize):\n",
        "    w, h = image.shape[1], image.shape[0]\n",
        "    w_new, h_new = process_resize(w, h, [resize,])\n",
        "\n",
        "    unit_shape = 8\n",
        "    w_new = w_new // unit_shape * unit_shape\n",
        "    h_new = h_new // unit_shape * unit_shape\n",
        "\n",
        "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
        "    image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
        "\n",
        "    inp = frame2tensor(image, \"cpu\")\n",
        "    return image, inp, scales, (h, w)\n",
        "\n",
        "class SGDataset(Dataset):\n",
        "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
        "        self.fnames1 = fnames1\n",
        "        self.fnames2 = fnames2\n",
        "        self.resize_to = resize_to\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname1 = self.fnames1[idx]\n",
        "        fname2 = self.fnames2[idx]\n",
        "\n",
        "        im1, im2 = cv2.imread(fname1, cv2.IMREAD_GRAYSCALE), cv2.imread(fname2, cv2.IMREAD_GRAYSCALE)\n",
        "        _, image1, scale1, ori_shape1 = sg_read_image(im1, self.device, self.resize_to)\n",
        "        _, image2, scale2, ori_shape2 = sg_read_image(im2, self.device, self.resize_to)\n",
        "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape1), torch.tensor(ori_shape2)\n",
        "\n",
        "def get_superglue_dataloader(images1, images2, resize_to, device, batch_size=1):\n",
        "    dataset = SGDataset(images1, images2, resize_to, device)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "def detect_superglue(\n",
        "    img_fnames, index_pairs, feature_dir, device, sg_config, file_keypoints,\n",
        "    resize_to=750, min_matches=15\n",
        "):\n",
        "    t=time()\n",
        "\n",
        "    matcher_superglue = Matching(sg_config).eval().to(device)\n",
        "\n",
        "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
        "    for pair_idx in progress_bar(index_pairs):\n",
        "        idx1, idx2 = pair_idx\n",
        "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "        fnames1.append(fname1)\n",
        "        fnames2.append(fname2)\n",
        "        idxs1.append(idx1)\n",
        "        idxs2.append(idx2)\n",
        "\n",
        "    dataloader = get_superglue_dataloader( fnames1, fnames2, resize_to, device)\n",
        "\n",
        "    cnt_pairs = 0\n",
        "\n",
        "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
        "        for X in dataloader:\n",
        "            image1, image2, idx, ori_shape_1, ori_shape_2 = X\n",
        "\n",
        "            fname1, fname2 = fnames1[idx], fnames2[idx]\n",
        "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
        "\n",
        "            pred = matcher_superglue({\"image0\": image1[0].to(device), \"image1\": image2[0].to(device)})\n",
        "            pred = {k: v[0].detach().cpu().numpy().copy() for k, v in pred.items()}\n",
        "            mkpts1, mkpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
        "            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
        "\n",
        "            valid = matches > -1\n",
        "            mkpts1 = mkpts1[valid]\n",
        "            mkpts2 = mkpts2[matches[valid]]\n",
        "            mconf = conf[valid]\n",
        "\n",
        "            ori_shape_1 = ori_shape_1[0].numpy()\n",
        "            ori_shape_2 = ori_shape_2[0].numpy()\n",
        "\n",
        "            # Scaling coords\n",
        "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / image1[0].shape[3]   # X\n",
        "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / image1[0].shape[2]   # Y\n",
        "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / image2[0].shape[3]   # X\n",
        "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / image2[0].shape[2]   # Y\n",
        "\n",
        "            n_matches = mconf.shape[0]\n",
        "\n",
        "            group  = f_match.require_group(key1)\n",
        "            if n_matches >= min_matches:\n",
        "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
        "                cnt_pairs+=1\n",
        "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(superglue)')\n",
        "            else:\n",
        "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
        "\n",
        "    gc.collect()\n",
        "    t=time() -t\n",
        "    print(f'Features matched in  {t:.4f} sec')\n",
        "    return t"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.565223Z",
          "iopub.execute_input": "2024-05-14T06:48:57.565962Z",
          "iopub.status.idle": "2024-05-14T06:48:57.630632Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.565931Z",
          "shell.execute_reply": "2024-05-14T06:48:57.629699Z"
        },
        "trusted": true,
        "id": "KslYR3hY6yL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoints: DKM"
      ],
      "metadata": {
        "id": "iJ13Itqg6yMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DKMDataset(Dataset):\n",
        "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
        "        self.fnames1 = fnames1\n",
        "        self.fnames2 = fnames2\n",
        "        self.resize_to = resize_to\n",
        "        self.device = device\n",
        "        self.test_transform = get_tuple_transform_ops(\n",
        "            resize=self.resize_to, normalize=True\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname1 = self.fnames1[idx]\n",
        "        fname2 = self.fnames2[idx]\n",
        "\n",
        "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
        "        ori_shape_1 = im1.size\n",
        "        ori_shape_2 = im2.size\n",
        "        image1, image2 = self.test_transform((im1, im2))\n",
        "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
        "\n",
        "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
        "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
        "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
        "    print(\"***\", dense_matches.shape, dense_certainty.shape)\n",
        "\n",
        "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
        "    # drop low confidence pairs\n",
        "    for b in range(dense_matches.shape[0]):\n",
        "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
        "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
        "\n",
        "        if u_dense_matches.shape[0] > num_features:\n",
        "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
        "\n",
        "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
        "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
        "\n",
        "        mkpts1 = u_dense_matches[:, :2]\n",
        "        mkpts2 = u_dense_matches[:, 2:]\n",
        "\n",
        "        w1, h1 = shapes1[b, :]\n",
        "        w2, h2 = shapes2[b, :]\n",
        "\n",
        "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
        "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
        "\n",
        "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
        "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
        "\n",
        "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
        "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
        "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        if mconf.shape[0] > min_matches:\n",
        "            try:\n",
        "                # calc Fundamental matrix from keypoints\n",
        "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
        "                inliers = inliers > 0\n",
        "                mkpts1 = mkpts1[inliers[:,0]]\n",
        "                mkpts2 = mkpts2[inliers[:,0]]\n",
        "                mconf  = mconf[inliers[:,0]]\n",
        "                #print(\"---\", mconf.shape)\n",
        "                if mconf.shape[0] > 3000:\n",
        "                    rand_idx = np.random.choice(range(mconf.shape[0]), 3000, replace=False)\n",
        "                    mkpts1 = mkpts1[rand_idx, :]\n",
        "                    mkpts2 = mkpts2[rand_idx, :]\n",
        "                    mconf  = mconf[rand_idx]\n",
        "            except:\n",
        "                mkpts1 = np.empty((0,2))\n",
        "                mkpts2 = np.empty((0,2))\n",
        "                mconf = np.empty((0,))\n",
        "\n",
        "        store_mkpts1.append(mkpts1)\n",
        "        store_mkpts2.append(mkpts2)\n",
        "        store_mconf.append(mconf)\n",
        "    return store_mkpts1, store_mkpts2, store_mconf\n",
        "\n",
        "def detect_dkm(\n",
        "    img_fnames, index_pairs, feature_dir, device,\n",
        "    resize_to=(540, 720),\n",
        "    detection_threshold=0.4,\n",
        "    num_features=2000,\n",
        "    min_matches=15,\n",
        "):\n",
        "    t=time()\n",
        "    dkm_model = DKMv3_outdoor(device=device)\n",
        "    dkm_model.upsample_preds=False\n",
        "\n",
        "    fnames1, fnames2 = [], []\n",
        "    for pair_idx in progress_bar(index_pairs):\n",
        "        idx1, idx2 = pair_idx\n",
        "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "        fnames1.append(fname1)\n",
        "        fnames2.append(fname2)\n",
        "\n",
        "    cnt_pairs = 0\n",
        "    with h5py.File(f'{feature_dir}/matches_dkm.h5', mode='w') as f_match:\n",
        "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
        "        for X in tqdm(dataloader):\n",
        "            images1, images2, idxs, shapes1, shapes2 = X\n",
        "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
        "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2,\n",
        "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
        "            )\n",
        "\n",
        "            for b in range(images1.shape[0]):\n",
        "                mkpts1 = store_mkpts1[b]\n",
        "                mkpts2 = store_mkpts2[b]\n",
        "                mconf = store_mconf[b]\n",
        "                file1 = fnames1[idxs[b]]\n",
        "                file2 = fnames2[idxs[b]]\n",
        "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
        "\n",
        "                n_matches = mconf.shape[0]\n",
        "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')\n",
        "\n",
        "                group  = f_match.require_group(key1)\n",
        "                if n_matches >= min_matches:\n",
        "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
        "                    cnt_pairs+=1\n",
        "    gc.collect()\n",
        "    t=time() -t\n",
        "    print(f'Features matched in  {t:.4f} sec')\n",
        "    return t"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.632086Z",
          "iopub.execute_input": "2024-05-14T06:48:57.632401Z",
          "iopub.status.idle": "2024-05-14T06:48:57.660504Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.632372Z",
          "shell.execute_reply": "2024-05-14T06:48:57.659601Z"
        },
        "trusted": true,
        "id": "dwLq1ipv6yMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoints: LoFTR"
      ],
      "metadata": {
        "id": "Iha8PiuH6yMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoFTRDataset(Dataset):\n",
        "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device):\n",
        "        self.fnames1 = fnames1\n",
        "        self.fnames2 = fnames2\n",
        "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
        "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
        "        self.idxs1 = idxs1\n",
        "        self.idxs2 = idxs2\n",
        "        self.resize_small_edge_to = resize_small_edge_to\n",
        "        self.device = device\n",
        "        self.round_unit = 16\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images1)\n",
        "\n",
        "    def load_torch_image(self, fname, device):\n",
        "        img = cv2.imread(fname)\n",
        "        original_shape = img.shape\n",
        "        ratio = self.resize_small_edge_to / min([img.shape[0], img.shape[1]])\n",
        "        w = int(img.shape[1] * ratio) # int( (img.shape[1] * ratio) // self.round_unit * self.round_unit )\n",
        "        h = int(img.shape[0] * ratio) # int( (img.shape[0] * ratio) // self.round_unit * self.round_unit )\n",
        "        img_resized = cv2.resize(img, (w, h))\n",
        "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
        "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
        "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
        "        return img_resized.to(device), original_shape\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname1 = self.fnames1[idx]\n",
        "        fname2 = self.fnames2[idx]\n",
        "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
        "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
        "\n",
        "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
        "\n",
        "def get_loftr_dataloader(images1, images2, idxs1, idxs2, resize_small_edge_to, device, batch_size=1):\n",
        "    dataset = LoFTRDataset(images1, images2, idxs1, idxs2, resize_small_edge_to, device)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def detect_loftr(img_fnames, index_pairs, feature_dir, device, file_keypoints, resize_small_edge_to=750, min_matches=15):\n",
        "    t=time()\n",
        "\n",
        "    matcher = LoFTR(pretrained=None)\n",
        "    matcher.load_state_dict(torch.load(\"../input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt\")['state_dict'])\n",
        "    matcher = matcher.to(device).eval()\n",
        "\n",
        "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
        "    for pair_idx in progress_bar(index_pairs):\n",
        "        idx1, idx2 = pair_idx\n",
        "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "        fnames1.append(fname1)\n",
        "        fnames2.append(fname2)\n",
        "        idxs1.append(idx1)\n",
        "        idxs2.append(idx2)\n",
        "\n",
        "\n",
        "    dataloader = get_loftr_dataloader( fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device)\n",
        "\n",
        "    cnt_pairs = 0\n",
        "\n",
        "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
        "        store_mkpts = {}\n",
        "        for X in tqdm(dataloader):\n",
        "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
        "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                correspondences = matcher( {\"image0\": image1.to(device),\"image1\": image2.to(device)} )\n",
        "                mkpts1 = correspondences['keypoints0'].cpu().numpy()\n",
        "                mkpts2 = correspondences['keypoints1'].cpu().numpy()\n",
        "                mconf  = correspondences['confidence'].cpu().numpy()\n",
        "\n",
        "            mkpts1[:,0] *= (float(ori_shape_1[1]) / float(image1.shape[3]))\n",
        "            mkpts1[:,1] *= (float(ori_shape_1[0]) / float(image1.shape[2]))\n",
        "\n",
        "            mkpts2[:,0] *= (float(ori_shape_2[1]) / float(image2.shape[3]))\n",
        "            mkpts2[:,1] *= (float(ori_shape_2[0]) / float(image2.shape[2]))\n",
        "\n",
        "            n_matches = mconf.shape[0]\n",
        "\n",
        "            group  = f_match.require_group(key1)\n",
        "            if n_matches >= min_matches:\n",
        "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
        "                cnt_pairs+=1\n",
        "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(loftr)')\n",
        "            else:\n",
        "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
        "    gc.collect()\n",
        "    t=time() -t\n",
        "    print(f'Features matched in  {t:.4f} sec')\n",
        "    return t"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.661843Z",
          "iopub.execute_input": "2024-05-14T06:48:57.662292Z",
          "iopub.status.idle": "2024-05-14T06:48:57.685655Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.662262Z",
          "shell.execute_reply": "2024-05-14T06:48:57.684589Z"
        },
        "trusted": true,
        "id": "OhMgdyeQ6yMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoints: DKM"
      ],
      "metadata": {
        "id": "jrsgSYf36yMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DKMDataset(Dataset):\n",
        "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
        "        self.fnames1 = fnames1\n",
        "        self.fnames2 = fnames2\n",
        "        self.resize_to = resize_to\n",
        "        self.device = device\n",
        "        self.test_transform = get_tuple_transform_ops(\n",
        "            resize=self.resize_to, normalize=True\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname1 = self.fnames1[idx]\n",
        "        fname2 = self.fnames2[idx]\n",
        "\n",
        "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
        "        ori_shape_1 = im1.size\n",
        "        ori_shape_2 = im2.size\n",
        "        image1, image2 = self.test_transform((im1, im2))\n",
        "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
        "\n",
        "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
        "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "        drop_last=False\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
        "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
        "\n",
        "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
        "    # drop low confidence pairs\n",
        "    for b in range(dense_matches.shape[0]):\n",
        "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
        "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
        "\n",
        "        if u_dense_matches.shape[0] > num_features:\n",
        "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
        "\n",
        "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
        "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
        "\n",
        "        mkpts1 = u_dense_matches[:, :2]\n",
        "        mkpts2 = u_dense_matches[:, 2:]\n",
        "\n",
        "        w1, h1 = shapes1[b, :]\n",
        "        w2, h2 = shapes2[b, :]\n",
        "\n",
        "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
        "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
        "\n",
        "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
        "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
        "\n",
        "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
        "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
        "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
        "\n",
        "        if mconf.shape[0] > min_matches:\n",
        "            try:\n",
        "                # calc Fundamental matrix from keypoints\n",
        "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
        "                inliers = inliers > 0\n",
        "                mkpts1 = mkpts1[inliers[:,0]]\n",
        "                mkpts2 = mkpts2[inliers[:,0]]\n",
        "                mconf  = mconf[inliers[:,0]]\n",
        "            except:\n",
        "                pass\n",
        "        store_mkpts1.append(mkpts1)\n",
        "        store_mkpts2.append(mkpts2)\n",
        "        store_mconf.append(mconf)\n",
        "    return store_mkpts1, store_mkpts2, store_mconf\n",
        "\n",
        "def detect_dkm(\n",
        "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
        "    resize_to=(540, 720),\n",
        "    detection_threshold=0.4,\n",
        "    num_features=2000,\n",
        "    min_matches=15\n",
        "):\n",
        "    t=time()\n",
        "    dkm_model = DKMv3_outdoor(device=device)\n",
        "    dkm_model.upsample_preds=False\n",
        "\n",
        "    fnames1, fnames2 = [], []\n",
        "    for pair_idx in progress_bar(index_pairs):\n",
        "        idx1, idx2 = pair_idx\n",
        "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "        fnames1.append(fname1)\n",
        "        fnames2.append(fname2)\n",
        "\n",
        "    cnt_pairs = 0\n",
        "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
        "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
        "        for X in tqdm(dataloader):\n",
        "            images1, images2, idxs, shapes1, shapes2 = X\n",
        "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
        "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2,\n",
        "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
        "            )\n",
        "\n",
        "            for b in range(images1.shape[0]):\n",
        "                mkpts1 = store_mkpts1[b]\n",
        "                mkpts2 = store_mkpts2[b]\n",
        "                mconf = store_mconf[b]\n",
        "                file1 = fnames1[idxs[b]]\n",
        "                file2 = fnames2[idxs[b]]\n",
        "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
        "\n",
        "                n_matches = mconf.shape[0]\n",
        "\n",
        "                group  = f_match.require_group(key1)\n",
        "                if n_matches >= min_matches:\n",
        "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
        "                    cnt_pairs+=1\n",
        "                    print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')\n",
        "                else:\n",
        "                    print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
        "\n",
        "    gc.collect()\n",
        "    t=time() -t\n",
        "    print(f'Features matched in  {t:.4f} sec')\n",
        "    return t"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.686879Z",
          "iopub.execute_input": "2024-05-14T06:48:57.687178Z",
          "iopub.status.idle": "2024-05-14T06:48:57.713379Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.68715Z",
          "shell.execute_reply": "2024-05-14T06:48:57.712463Z"
        },
        "trusted": true,
        "id": "-TylfZEN6yMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keypoints merger"
      ],
      "metadata": {
        "id": "Zs6NmtE06yMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_idxs(A, dim=0):\n",
        "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
        "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
        "    _, ind_sorted = torch.sort(idx, stable=True)\n",
        "    cum_sum = counts.cumsum(0)\n",
        "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
        "    first_indices = ind_sorted[cum_sum]\n",
        "    return first_indices\n",
        "\n",
        "def get_keypoint_from_h5(fp, key1, key2):\n",
        "    rc = -1\n",
        "    try:\n",
        "        kpts = np.array(fp[key1][key2])\n",
        "        rc = 0\n",
        "        return (rc, kpts)\n",
        "    except:\n",
        "        return (rc, None)\n",
        "\n",
        "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
        "    list_mkpts = []\n",
        "    for fp in fps:\n",
        "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
        "        if rc == 0:\n",
        "            list_mkpts.append(mkpts)\n",
        "    if len(list_mkpts) > 0:\n",
        "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
        "    else:\n",
        "        list_mkpts = None\n",
        "    return list_mkpts\n",
        "\n",
        "def keypoints_merger(\n",
        "    img_fnames,\n",
        "    index_pairs,\n",
        "    files_keypoints,\n",
        "    feature_dir = 'featureout',\n",
        "):\n",
        "    # open h5 files\n",
        "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
        "\n",
        "    # temprary file\n",
        "    with h5py.File(f'{feature_dir}/merge_tmp.h5', mode='w') as f_match:\n",
        "        counter = 0\n",
        "        for pair_idx in progress_bar(index_pairs):\n",
        "            idx1, idx2 = pair_idx\n",
        "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
        "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
        "\n",
        "            # extract keypoints\n",
        "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
        "            if mkpts is None:\n",
        "                print(f\"skipped key1={key1}, key2={key2}\")\n",
        "                continue\n",
        "\n",
        "            print (f'{key1}-{key2}: {mkpts.shape[0]} matches')\n",
        "            # regist tmp file\n",
        "            group  = f_match.require_group(key1)\n",
        "            group.create_dataset(key2, data=mkpts)\n",
        "            counter += 1\n",
        "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
        "    for fp in fps:\n",
        "        fp.close()\n",
        "\n",
        "    # Let's find unique loftr pixels and group them together.\n",
        "    kpts = defaultdict(list)\n",
        "    match_indexes = defaultdict(dict)\n",
        "    total_kpts=defaultdict(int)\n",
        "    with h5py.File(f'{feature_dir}/merge_tmp.h5', mode='r') as f_match:\n",
        "        for k1 in f_match.keys():\n",
        "            group  = f_match[k1]\n",
        "            for k2 in group.keys():\n",
        "                matches = group[k2][...]\n",
        "                total_kpts[k1]\n",
        "                kpts[k1].append(matches[:, :2])\n",
        "                kpts[k2].append(matches[:, 2:])\n",
        "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
        "                current_match[:, 0]+=total_kpts[k1]\n",
        "                current_match[:, 1]+=total_kpts[k2]\n",
        "                total_kpts[k1]+=len(matches)\n",
        "                total_kpts[k2]+=len(matches)\n",
        "                match_indexes[k1][k2]=current_match\n",
        "\n",
        "    for k in kpts.keys():\n",
        "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
        "    unique_kpts = {}\n",
        "    unique_match_idxs = {}\n",
        "    out_match = defaultdict(dict)\n",
        "    for k in kpts.keys():\n",
        "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
        "        unique_match_idxs[k] = uniq_reverse_idxs\n",
        "        unique_kpts[k] = uniq_kps.numpy()\n",
        "    for k1, group in match_indexes.items():\n",
        "        for k2, m in group.items():\n",
        "            m2 = deepcopy(m)\n",
        "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
        "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
        "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
        "                                    unique_kpts[k2][  m2[:,1]],\n",
        "                                   ],\n",
        "                                   axis=1)\n",
        "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
        "            m2_semiclean = m2[unique_idxs_current]\n",
        "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
        "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
        "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
        "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
        "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
        "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
        "        for k, kpts1 in unique_kpts.items():\n",
        "            f_kp[k] = kpts1\n",
        "\n",
        "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
        "        for k1, gr in out_match.items():\n",
        "            group  = f_match.require_group(k1)\n",
        "            for k2, match in gr.items():\n",
        "                group[k2] = match\n",
        "    return"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.714669Z",
          "iopub.execute_input": "2024-05-14T06:48:57.714958Z",
          "iopub.status.idle": "2024-05-14T06:48:57.739871Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.714935Z",
          "shell.execute_reply": "2024-05-14T06:48:57.738949Z"
        },
        "trusted": true,
        "id": "KkBkNvEs6yMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission utilities"
      ],
      "metadata": {
        "id": "kXZPZYzR6yMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arr_to_str(a):\n",
        "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
        "\n",
        "# Function to create a submission file.\n",
        "def create_submission(out_results, data_dict):\n",
        "    with open(f'submission.csv', 'w') as f:\n",
        "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
        "        for dataset in data_dict:\n",
        "            if dataset in out_results:\n",
        "                res = out_results[dataset]\n",
        "            else:\n",
        "                res = {}\n",
        "            for scene in data_dict[dataset]:\n",
        "                if scene in res:\n",
        "                    scene_res = res[scene]\n",
        "                else:\n",
        "                    scene_res = {\"R\":{}, \"t\":{}}\n",
        "                for image in data_dict[dataset][scene]:\n",
        "                    if image in scene_res:\n",
        "                        print (image)\n",
        "                        R = scene_res[image]['R'].reshape(-1)\n",
        "                        T = scene_res[image]['t'].reshape(-1)\n",
        "                    else:\n",
        "                        R = np.eye(3).reshape(-1)\n",
        "                        T = np.zeros((3))\n",
        "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.741046Z",
          "iopub.execute_input": "2024-05-14T06:48:57.741389Z",
          "iopub.status.idle": "2024-05-14T06:48:57.751932Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.741359Z",
          "shell.execute_reply": "2024-05-14T06:48:57.75121Z"
        },
        "trusted": true,
        "id": "HtDomlvI6yMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "jtanOt8O6yMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = '/kaggle/input/image-matching-challenge-2024'\n",
        "DEBUG = False\n",
        "DUMP = False\n",
        "\n",
        "# Get data from csv.\n",
        "data_dict = {}\n",
        "with open(f'{src}/sample_submission.csv', 'r') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        # Skip header.\n",
        "        if l and i > 0:\n",
        "            image, dataset, scene, _, _ = l.strip().split(',')\n",
        "            if dataset not in data_dict:\n",
        "                data_dict[dataset] = {}\n",
        "            if scene not in data_dict[dataset]:\n",
        "                data_dict[dataset][scene] = []\n",
        "            data_dict[dataset][scene].append(image)\n",
        "\n",
        "            #if len(data_dict[dataset][scene]) == 21:\n",
        "            #    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.752836Z",
          "iopub.execute_input": "2024-05-14T06:48:57.753169Z",
          "iopub.status.idle": "2024-05-14T06:48:57.769196Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.753145Z",
          "shell.execute_reply": "2024-05-14T06:48:57.768389Z"
        },
        "trusted": true,
        "id": "JGF0Xcy56yMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in data_dict:\n",
        "    for scene in data_dict[dataset]:\n",
        "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.770589Z",
          "iopub.execute_input": "2024-05-14T06:48:57.770882Z",
          "iopub.status.idle": "2024-05-14T06:48:57.778733Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.77086Z",
          "shell.execute_reply": "2024-05-14T06:48:57.777625Z"
        },
        "trusted": true,
        "id": "PfzESSLm6yMB",
        "outputId": "2299e47c-2ac7-430d-e587-218536851a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "church / church -> 41 images\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_results = {}\n",
        "timings = {\"shortlisting\":[],\n",
        "           \"feature_detection\": [],\n",
        "           \"feature_matching\":[],\n",
        "           \"RANSAC\": [],\n",
        "           \"Reconstruction\": []}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.779884Z",
          "iopub.execute_input": "2024-05-14T06:48:57.780212Z",
          "iopub.status.idle": "2024-05-14T06:48:57.784991Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.780184Z",
          "shell.execute_reply": "2024-05-14T06:48:57.78415Z"
        },
        "trusted": true,
        "id": "e2q-YcME6yMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "datasets = []\n",
        "for dataset in data_dict:\n",
        "    datasets.append(dataset)\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    if dataset not in out_results:\n",
        "        out_results[dataset] = {}\n",
        "    for scene in data_dict[dataset]:\n",
        "        print(scene)\n",
        "        # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
        "        # You may want to run this on the training data in that case?\n",
        "        img_dir = f'{src}/test/{dataset}/images'\n",
        "        if not os.path.exists(img_dir):\n",
        "            continue\n",
        "        # Wrap the meaty part in a try-except block.\n",
        "        try:\n",
        "            out_results[dataset][scene] = {}\n",
        "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
        "            print (f\"Got {len(img_fnames)} images\")\n",
        "            feature_dir = f'featureout/{dataset}_{scene}'\n",
        "            if not os.path.isdir(feature_dir):\n",
        "                os.makedirs(feature_dir, exist_ok=True)\n",
        "\n",
        "            #############################################################\n",
        "            # get image pairs\n",
        "            #############################################################\n",
        "            t=time()\n",
        "            index_pairs = get_image_pairs_shortlist(img_fnames,\n",
        "                                  sim_th = 1.0, # should be strict\n",
        "                                  min_pairs = 50, # we select at least min_pairs PER IMAGE with biggest similarity\n",
        "                                  exhaustive_if_less = 50,\n",
        "                                  device=device)\n",
        "            t=time() -t\n",
        "            timings['shortlisting'].append(t)\n",
        "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
        "            gc.collect()\n",
        "\n",
        "            #############################################################\n",
        "            # get keypoints\n",
        "            #############################################################\n",
        "            files_keypoints = []\n",
        "            if CONFIG.use_superglue:\n",
        "                resize_to = CONFIG.params_sg[\"resize_to\"]\n",
        "                file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
        "                !rm -rf {file_keypoints}\n",
        "                t = detect_superglue(\n",
        "                    img_fnames, index_pairs, feature_dir, device,\n",
        "                    CONFIG.params_sg[\"sg_config\"], file_keypoints,\n",
        "                    resize_to=CONFIG.params_sg[\"resize_to\"],\n",
        "                    min_matches=CONFIG.params_sg[\"min_matches\"],\n",
        "                )\n",
        "                gc.collect()\n",
        "                files_keypoints.append( file_keypoints )\n",
        "                timings['feature_matching'].append(t)\n",
        "\n",
        "            if CONFIG.use_aliked_lightglue:\n",
        "                model_name = \"aliked\"\n",
        "                file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
        "                t = detect_lightglue_common(\n",
        "                    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints,\n",
        "                    resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
        "                    detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
        "                    num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
        "                    min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
        "                )\n",
        "                gc.collect()\n",
        "                files_keypoints.append(file_keypoints)\n",
        "                timings['feature_matching'].append(t)\n",
        "\n",
        "            if CONFIG.use_doghardnet_lightglue:\n",
        "                model_name = \"doghardnet\"\n",
        "                file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
        "                t = detect_lightglue_common(\n",
        "                    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints,\n",
        "                    resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
        "                    detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
        "                    num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
        "                    min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
        "                )\n",
        "                gc.collect()\n",
        "                files_keypoints.append(file_keypoints)\n",
        "                timings['feature_matching'].append(t)\n",
        "\n",
        "            if CONFIG.use_superpoint_lightglue:\n",
        "                model_name = \"superpoint\"\n",
        "                file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
        "                t = detect_lightglue_common(\n",
        "                    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints,\n",
        "                    resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
        "                    detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
        "                    num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
        "                    min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
        "                )\n",
        "                gc.collect()\n",
        "                files_keypoints.append(file_keypoints)\n",
        "                timings['feature_matching'].append(t)\n",
        "\n",
        "            if CONFIG.use_loftr:\n",
        "                file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
        "                t = detect_loftr(\n",
        "                    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
        "                    resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
        "                    min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
        "                )\n",
        "                gc.collect()\n",
        "                files_keypoints.append( file_keypoints )\n",
        "                timings['feature_matching'].append(t)\n",
        "\n",
        "            if CONFIG.use_dkm:\n",
        "                file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
        "                t = detect_dkm(\n",
        "                    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
        "                    resize_to=CONFIG.params_dkm[\"resize_to\"],\n",
        "                    detection_threshold=CONFIG.params_dkm[\"detection_threshold\"],\n",
        "                    num_features=CONFIG.params_dkm[\"num_features\"],\n",
        "                    min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
        "                )\n",
        "                gc.collect()\n",
        "                files_keypoints.append(file_keypoints)\n",
        "                timings['feature_matching'].append(t)\n",
        "\n",
        "            #############################################################\n",
        "            # merge keypoints\n",
        "            #############################################################\n",
        "            keypoints_merger(\n",
        "                img_fnames,\n",
        "                index_pairs,\n",
        "                files_keypoints,\n",
        "                feature_dir = feature_dir,\n",
        "            )\n",
        "\n",
        "            #############################################################\n",
        "            # regist keypoints from h5 into colmap db\n",
        "            #############################################################\n",
        "            database_path = f'{feature_dir}/colmap.db'\n",
        "            if os.path.isfile(database_path):\n",
        "                os.remove(database_path)\n",
        "            gc.collect()\n",
        "            import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n",
        "            output_path = f'{feature_dir}/colmap_rec'\n",
        "\n",
        "            #############################################################\n",
        "            # Calculate fundamental matrix with colmap api\n",
        "            #############################################################\n",
        "            t=time()\n",
        "            options = pycolmap.SiftMatchingOptions()\n",
        "            options.confidence = 0.9999\n",
        "            options.max_num_trials = 20000\n",
        "            pycolmap.match_exhaustive(database_path, sift_options=options)\n",
        "            t=time() - t\n",
        "            timings['RANSAC'].append(t)\n",
        "            print(f'RANSAC in  {t:.4f} sec')\n",
        "\n",
        "            #############################################################\n",
        "            # Execute bundle adjustmnet with colmap api\n",
        "            # --> Bundle adjustment Calcs Camera matrix, R and t\n",
        "            #############################################################\n",
        "            t=time()\n",
        "            # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
        "            mapper_options = pycolmap.IncrementalMapperOptions()\n",
        "            mapper_options.min_model_size = 3\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "            maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
        "            print(maps)\n",
        "            clear_output(wait=False)\n",
        "            t=time() - t\n",
        "            timings['Reconstruction'].append(t)\n",
        "            print(f'Reconstruction done in  {t:.4f} sec')\n",
        "\n",
        "            #############################################################\n",
        "            # Extract R,t from maps\n",
        "            #############################################################\n",
        "            imgs_registered  = 0\n",
        "            best_idx = None\n",
        "            list_num_images = []\n",
        "            print (\"Looking for the best reconstruction\")\n",
        "            if isinstance(maps, dict):\n",
        "                for idx1, rec in maps.items():\n",
        "                    print (idx1, rec.summary())\n",
        "                    list_num_images.append( len(rec.images) )\n",
        "                    if len(rec.images) > imgs_registered:\n",
        "                        imgs_registered = len(rec.images)\n",
        "                        best_idx = idx1\n",
        "            list_num_images = np.array(list_num_images)\n",
        "            print(f\"list_num_images = {list_num_images}\")\n",
        "            if best_idx is not None:\n",
        "                print (maps[best_idx].summary())\n",
        "                for k, im in maps[best_idx].images.items():\n",
        "                    key1 = f'test/{dataset}/images/{im.name}'\n",
        "                    out_results[dataset][scene][key1] = {}\n",
        "                    out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n",
        "                    out_results[dataset][scene][key1][\"t\"] = deepcopy(np.array(im.tvec))\n",
        "\n",
        "            print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n",
        "            print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
        "            create_submission(out_results, data_dict)\n",
        "            gc.collect()\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T06:48:57.786089Z",
          "iopub.execute_input": "2024-05-14T06:48:57.786348Z",
          "iopub.status.idle": "2024-05-14T07:04:29.321297Z",
          "shell.execute_reply.started": "2024-05-14T06:48:57.786325Z",
          "shell.execute_reply": "2024-05-14T07:04:29.320377Z"
        },
        "trusted": true,
        "id": "jayZ1mSN6yMC",
        "outputId": "0bb55bb9-715e-4c11-b3c1-5d2ff8b23bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Reconstruction done in  318.2972 sec\nLooking for the best reconstruction\n0 Reconstruction:\n\tnum_reg_images = 39\n\tnum_cameras = 39\n\tnum_points3D = 22272\n\tnum_observations = 129858\n\tmean_track_length = 5.83055\n\tmean_observations_per_image = 3329.69\n\tmean_reprojection_error = 0.949581\nlist_num_images = [39]\nReconstruction:\n\tnum_reg_images = 39\n\tnum_cameras = 39\n\tnum_points3D = 22272\n\tnum_observations = 129858\n\tmean_track_length = 5.83055\n\tmean_observations_per_image = 3329.69\n\tmean_reprojection_error = 0.949581\nRegistered: church / church -> 39 images\nTotal: church / church -> 41 images\ntest/church/images/00046.png\ntest/church/images/00090.png\ntest/church/images/00092.png\ntest/church/images/00087.png\ntest/church/images/00050.png\ntest/church/images/00068.png\ntest/church/images/00083.png\ntest/church/images/00096.png\ntest/church/images/00069.png\ntest/church/images/00081.png\ntest/church/images/00042.png\ntest/church/images/00018.png\ntest/church/images/00030.png\ntest/church/images/00024.png\ntest/church/images/00032.png\ntest/church/images/00026.png\ntest/church/images/00037.png\ntest/church/images/00008.png\ntest/church/images/00035.png\ntest/church/images/00021.png\ntest/church/images/00010.png\ntest/church/images/00039.png\ntest/church/images/00011.png\ntest/church/images/00013.png\ntest/church/images/00006.png\ntest/church/images/00012.png\ntest/church/images/00029.png\ntest/church/images/00001.png\ntest/church/images/00098.png\ntest/church/images/00072.png\ntest/church/images/00066.png\ntest/church/images/00058.png\ntest/church/images/00059.png\ntest/church/images/00111.png\ntest/church/images/00061.png\ntest/church/images/00074.png\ntest/church/images/00102.png\ntest/church/images/00076.png\ntest/church/images/00063.png\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result"
      ],
      "metadata": {
        "id": "IiAA1Li86yMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat submission.csv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-14T07:04:29.322515Z",
          "iopub.execute_input": "2024-05-14T07:04:29.322824Z"
        },
        "trusted": true,
        "id": "JJ_zPysW6yMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}