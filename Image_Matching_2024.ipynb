{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 71885,
          "databundleVersionId": 8143495,
          "sourceType": "competition"
        },
        {
          "sourceId": 7884485,
          "sourceType": "datasetVersion",
          "datasetId": 4628051
        },
        {
          "sourceId": 7884725,
          "sourceType": "datasetVersion",
          "datasetId": 4628331
        },
        {
          "sourceId": 4534,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3326
        },
        {
          "sourceId": 17191,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 14317
        },
        {
          "sourceId": 17555,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 14611
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Image Matching 2024",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktichola/Image-matching/blob/main/Image_Matching_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'image-matching-challenge-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F71885%2F8143495%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T073243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5f36085f84a8e7f25aecb5d040c82f153ac36e83a651e60eba4b9ddb7f73ad1e40d445ebff92d785f9caf31949535f5f0f78ea7dce2cf3c3a3b8cfeb567d650f539ce5c56a2f419004e1c92f6cdc810e9b943409c5b038e28323ec9734b001b95021449d848ff42cf93139a8c466d5583696f786303778353f56f34dd941801358a71b0cb98ea4af7d023efcb442a7d56fb63d9b75a5be5ee5a1d56b357b72d59de33de244ca3510b4c57295e7a23c3728a1e2e25bf08c1cca37c03b2a21761e4241b0ebb0ade848f92cbb81587d038f555778ca64631c7e62ca6af57910a280ed4b170c284b3c1bfbf0d0b56953a0ff7d2990b245ae4a1e124d20dcc0c3098b,imc2024-packages-lightglue-rerun-kornia:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4628051%2F7884485%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T073243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9e3a9c43a814d226ce3e743cfa6bf924029a4b17bb47f5e1b0327e38d39190111cee6f192ac4e3c7e49023cbdcb371e2bc6dadaefe81af2e7aea0cf024213e008188db1fcd612da911296fa0f6fa2fd752c6784ef092d59886e0bb1556de59ce31e98db4b995d1a0fbfe7d1b8b1ba5e8b57a273a87b6bf08f1ea8fc2e6cef3249fa75061ee61bc6bd5d3f0f4b55f063a7cc4f61ee69a43ba01c1bc42bda5beff6ad0052115507a386c4483799173caa2849790c4e243632fcce440eef0ca84a5e328dbdec90b7f29d3d0a8b62ceb2ce815b089cb490485d9939f9360a5552f6ceac1be2747edb3d2f1a9e279087ea5a47ff21b68a5aa0253ef523f5e51b402ec,colmap-db-import:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4628331%2F7884725%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T073243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D86408ca547f556ce10acfe8a095371f395845a02dc57e0e1bc027b74d55b04dd7cee489286844af1b0c85d0eb73fbd72b9e80b93e39b2a2e52057464cdf9dacd50f71e3bc16dee03b2c165cbb75ac06b15e87b4758aceaac6d6d9d51ec7966d5f0b5be60db58212f61e79bfd198db9c87a6e3868892845c718c96cbf9180955afa92eb6c3f3ad1ffe01d6663ab7826522a138459e2e816b04b267649ea0618cda27824959808c7fea58830695efe08223630176df43b945bf5f151e48d315e8aea22ba2f9ebaaae8ec053cf91f35a7940d1134be61e0d3ac2958a70f7a9ff831e43bc6c34aaf17a599f8c7faebc4e05ca796020ccd25df04fd903997bbca0eb3,dinov2/pytorch/base/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3326%2F4534%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T073243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7615615e9cf81ee93f0f1c70b484238b209cfd7042e975cd5492e359fb15ef872d2ae9dfed3398e034dc765e23027a51659e39e24cd689de700aabfb4014aa276ca7d4997d0a1f2350200d4c1f9ea475da28be8125526143731a8e2e391c63eb3a6394a54b7331ee05102f275f547cb4ae80c6fec405f149daa68db2a29bdcc099954b758aba80f88a3b25d22a90ce40f1bdd136c307a6cbf66440640b6f6e81c6bc2de92f420d5e61ed1b351ca4a0de1ec01daba4d957ef8045ab317118b58eb48019802d2e61ba88b905b5ab9db947d24e028139e86dd8a72489f20506e58302a287d40f5664b89180baca2b56063755e3ac85b16c9e3231d8c72e5d56db6c,lightglue/pytorch/aliked/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F14317%2F17191%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T073243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D90c9b04232741934f049aca731d63282a3e60ced0e20980451224fc0af3a70cd330653ecd69c60ba796cd03f131bfe29055e3541d47e5424ca188f02e0fb757e84bf26b585f4d4c945d1934ed1ed28fa8414d09df2da8ac9b6384db8180d5f61a17ee89bc804da4e2a036532dcfa9e5e95729f8f9c2fa32c75dda402dac162db1ef08daa62a197a18ee0645b28ff289468e379107595fe911fe62ff3351d864499dabe939f8327f72b3c6e61ba3543412193ad52eeaf198419fb32f3a1747a962f543e9851fe3404a9caeb856d689228170b54e628ab7430269a0be68d015afb0e2bebb67ff205ea89b3d91b72cf9742af2f0efc00747e91650fba47452432a7,aliked/pytorch/aliked-n16/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F14611%2F17555%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T073243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0fd2e6938b12d91872a35b23fada60dfcdc9afa03be431cd0a3980cfc17288851cb495f1689b1bddc0afa4a5de0709cd4fe1c789a430bb84601dde14f83c63323e05356029721231387074eb54299803d980dae59da3c36a353798e0506dd5050ab408d4c06ef0654ba315ad7821373dc285a05d2a9d9891ff83e1ac7ee9bc0bd60a19504b4978b5194f8954219d7e9f3ef2ea7a9df396b34cf7d769c8da27f7826ab0e2b2fc71f5b3568b1e75b6155864b4eaca5e5ea5a8921da509576e347d5e9fa3fe8cc8a7b2c95b1808548c4cf4e14195e6cc103166a6d4fa8301003597b259791511cdffbb56187aed9da163d1e0b0714f1f152843c36ee7e201b83e2e'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "sN4lud6wBJsA"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
        "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
        "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
        "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
        "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
      ],
      "metadata": {
        "trusted": true,
        "id": "AaS3W76cBJsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from time import time, sleep\n",
        "from fastprogress import progress_bar\n",
        "import gc\n",
        "import numpy as np\n",
        "import h5py\n",
        "from IPython.display import clear_output\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "from typing import Any\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# CV/MLe\n",
        "import cv2\n",
        "import torch\n",
        "from torch import Tensor as T\n",
        "import torch.nn.functional as F\n",
        "import kornia as K\n",
        "import kornia.feature as KF\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "import torch\n",
        "from lightglue import match_pair\n",
        "from lightglue import LightGlue, ALIKED\n",
        "from lightglue.utils import load_image, rbd\n",
        "\n",
        "# 3D reconstruction\n",
        "import pycolmap\n",
        "\n",
        "# Data importing into colmap\n",
        "import sys\n",
        "sys.path.append(\"/kaggle/input/colmap-db-import\")\n",
        "\n",
        "# Provided by organizers\n",
        "from database import *\n",
        "from h5_to_db import *\n",
        "\n",
        "def arr_to_str(a):\n",
        "    \"\"\"Returns ;-separated string representing the input\"\"\"\n",
        "    return \";\".join([str(x) for x in a.reshape(-1)])\n",
        "\n",
        "def load_torch_image(file_name: Path | str, device=torch.device(\"cpu\")):\n",
        "    \"\"\"Loads an image and adds batch dimension\"\"\"\n",
        "    img = K.io.load_image(file_name, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
        "    return img\n",
        "\n",
        "\n",
        "def embed_images(\n",
        "    paths: list[Path],\n",
        "    model_name: str,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") :\n",
        "    \"\"\"Computes image embeddings.\n",
        "\n",
        "    Returns a tensor of shape [len(filenames), output_dim]\n",
        "    \"\"\"\n",
        "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).eval().to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for i, path in tqdm(enumerate(paths), desc=\"Global descriptors\"):\n",
        "        image = load_torch_image(path)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            inputs = processor(images=image, return_tensors=\"pt\", do_rescale=False).to(device)\n",
        "            outputs = model(**inputs) # last_hidden_state and pooled\n",
        "\n",
        "            # Max pooling over all the hidden states but the first (starting token)\n",
        "            # To obtain a tensor of shape [1, output_dim]\n",
        "            # We normalize so that distances are computed in a better fashion later\n",
        "            embedding = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=-1, p=2)\n",
        "\n",
        "        embeddings.append(embedding.detach().cpu())\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "\n",
        "def load_torch_image(file_name: Path | str, device=torch.device(\"cpu\")):\n",
        "    \"\"\"Loads an image and adds batch dimension\"\"\"\n",
        "    img = K.io.load_image(file_name, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
        "    return img\n",
        "\n",
        "def get_image_pairs(paths: list[Path],model_name: str,similarity_threshold: float = 0.6,\n",
        "                    tolerance: int = 1000,min_matches: int = 20,exhaustive_if_less: int = 20,\n",
        "                    p: float = 2.0,device: torch.device = torch.device(\"cpu\")):\n",
        "    # We try to minimize the camera to reconstruct the scene due to saving computing cost and minimizing the error.\n",
        "    if len(paths) <= exhaustive_if_less:\n",
        "        return get_pairs_exhaustive(paths)\n",
        "    matches = []\n",
        "    embeddings = embed_images(paths, model_name)\n",
        "    distances = torch.cdist(embeddings, embeddings, p=p)\n",
        "\n",
        "    # Remove pairs above similarity threshold (if enough)\n",
        "    mask = distances <= similarity_threshold\n",
        "    image_indices = np.arange(len(paths))\n",
        "\n",
        "    for current_image_index in range(len(paths)):\n",
        "        mask_row = mask[current_image_index]\n",
        "        indices_to_match = image_indices[mask_row]\n",
        "\n",
        "        # We don't have enough matches below the threshold, we pick most similar ones\n",
        "        if len(indices_to_match) < min_matches:\n",
        "            indices_to_match = np.argsort(distances[current_image_index])[:min_matches]\n",
        "\n",
        "        for other_image_index in indices_to_match:\n",
        "            # Skip an image matching itself\n",
        "            if other_image_index == current_image_index:\n",
        "                continue\n",
        "\n",
        "            # We need to check if we are below a certain distance tolerance\n",
        "            # since for images that don't have enough matches, we picked\n",
        "            # the most similar ones (which could all still be very different\n",
        "            # to the image we are analyzing)\n",
        "            if distances[current_image_index, other_image_index] < tolerance:\n",
        "                # Add the pair in a sorted manner to avoid redundancy\n",
        "                matches.append(tuple(sorted((current_image_index, other_image_index.item()))))\n",
        "\n",
        "    return sorted(list(set(matches)))\n",
        "\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "8oN20ZLsBJsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_keypoints(\n",
        "    paths: list[Path],\n",
        "    feature_dir: Path,\n",
        "    num_features: int = 4096,\n",
        "    resize_to: int = 1024,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") -> None:\n",
        "    \"\"\"Detects the keypoints in a list of images with ALIKED\n",
        "\n",
        "    Stores them in feature_dir/keypoints.h5 and feature_dir/descriptors.h5\n",
        "    to be used later with LightGlue\n",
        "    \"\"\"\n",
        "    dtype = torch.float32 # ALIKED has issues with float16\n",
        "\n",
        "    extractor = ALIKED(\n",
        "        max_num_keypoints=num_features,\n",
        "        detection_threshold=0.01,\n",
        "        resize=resize_to\n",
        "    ).eval().to(device, dtype)\n",
        "\n",
        "    feature_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"w\") as f_keypoints, \\\n",
        "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"w\") as f_descriptors:\n",
        "\n",
        "        for path in tqdm(paths, desc=\"Computing keypoints\"):\n",
        "            key = path.name\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                image = load_torch_image(path, device=device).to(dtype)\n",
        "                features = extractor.extract(image)\n",
        "\n",
        "                f_keypoints[key] = features[\"keypoints\"].squeeze().detach().cpu().numpy()\n",
        "                f_descriptors[key] = features[\"descriptors\"].squeeze().detach().cpu().numpy()"
      ],
      "metadata": {
        "trusted": true,
        "id": "B4_eSBzlBJsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keypoint_distances(\n",
        "    paths: list[Path],\n",
        "    index_pairs: list[tuple[int, int]],\n",
        "    feature_dir: Path,\n",
        "    min_matches: int = 15,\n",
        "    verbose: bool = True,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") -> None:\n",
        "    \"\"\"Computes distances between keypoints of images.\n",
        "\n",
        "    Stores output at feature_dir/matches.h5\n",
        "    \"\"\"\n",
        "\n",
        "    matcher_params = {\n",
        "        \"width_confidence\": -1,\n",
        "        \"depth_confidence\": -1,\n",
        "        \"mp\": True if 'cuda' in str(device) else False,\n",
        "    }\n",
        "    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n",
        "\n",
        "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n",
        "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors, \\\n",
        "         h5py.File(feature_dir / \"matches.h5\", mode=\"w\") as f_matches:\n",
        "\n",
        "            for idx1, idx2 in tqdm(index_pairs, desc=\"Computing keypoing distances\"):\n",
        "                key1, key2 = paths[idx1].name, paths[idx2].name\n",
        "\n",
        "                keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n",
        "                keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n",
        "                descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n",
        "                descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n",
        "\n",
        "                with torch.inference_mode():\n",
        "                    distances, indices = matcher(\n",
        "                        descriptors1,\n",
        "                        descriptors2,\n",
        "                        KF.laf_from_center_scale_ori(keypoints1[None]),\n",
        "                        KF.laf_from_center_scale_ori(keypoints2[None]),\n",
        "                    )\n",
        "\n",
        "                # We have matches to consider\n",
        "                n_matches = len(indices)\n",
        "                if n_matches:\n",
        "                    if verbose:\n",
        "                        print(f\"{key1}-{key2}: {n_matches} matches\")\n",
        "                # Store the matches in the group of one image\n",
        "                group  = f_matches.require_group(key1)\n",
        "                if n_matches >= min_matches:\n",
        "                     group.create_dataset(key2, data=indices.detach().cpu().numpy().reshape(-1, 2))"
      ],
      "metadata": {
        "trusted": true,
        "id": "cApIhClrBJsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    base_path: Path = Path(\"/kaggle/input/image-matching-challenge-2024\")\n",
        "    feature_dir: Path = Path.cwd() / \".feature_outputs\"\n",
        "    device: torch.device= K.utils.get_cuda_device_if_available(0)\n",
        "    pair_matching_args = {\n",
        "        \"model_name\": \"/kaggle/input/dinov2/pytorch/base/1\",\n",
        "        \"similarity_threshold\": 0.3,\n",
        "        \"tolerance\": 1000,\n",
        "        \"min_matches\": 20,\n",
        "        \"exhaustive_if_less\": 20,\n",
        "        \"p\": 2.0\n",
        "    }\n",
        "    keypoint_detection_args = {\n",
        "        \"num_features\": 4096,\n",
        "        \"resize_to\": 1024,\n",
        "    }\n",
        "\n",
        "    keypoint_distances_args = {\n",
        "        \"min_matches\": 15,\n",
        "        \"verbose\": False,\n",
        "    }\n",
        "    colmap_mapper_options = {\n",
        "        \"min_model_size\": 3, # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
        "        \"max_num_models\": 2,\n",
        "    }"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-input": false,
        "trusted": true,
        "id": "PoFTLAjYBJsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sample_submission(\n",
        "    base_path: Path,\n",
        ") -> dict[dict[str, list[Path]]]:\n",
        "    \"\"\"Construct a dict describing the test data as\n",
        "\n",
        "    {\"dataset\": {\"scene\": [<image paths>]}}\n",
        "    \"\"\"\n",
        "    data_dict = {}\n",
        "    with open(base_path / \"sample_submission.csv\", \"r\") as f:\n",
        "        for i, l in enumerate(f):\n",
        "            # Skip header\n",
        "            if i == 0:\n",
        "                print(\"header:\", l)\n",
        "\n",
        "            if l and i > 0:\n",
        "                image_path, dataset, scene, _, _ = l.strip().split(',')\n",
        "                if dataset not in data_dict:\n",
        "                    data_dict[dataset] = {}\n",
        "                if scene not in data_dict[dataset]:\n",
        "                    data_dict[dataset][scene] = []\n",
        "                data_dict[dataset][scene].append(Path(base_path / image_path))\n",
        "\n",
        "    for dataset in data_dict:\n",
        "        for scene in data_dict[dataset]:\n",
        "            print(f\"{dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
        "\n",
        "    return data_dict"
      ],
      "metadata": {
        "trusted": true,
        "id": "uL5XoF8NBJsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_submission(\n",
        "    results: dict,\n",
        "    data_dict: dict[dict[str, list[Path]]],\n",
        "    base_path: Path,\n",
        ") -> None:\n",
        "    \"\"\"Prepares a submission file.\"\"\"\n",
        "\n",
        "    with open(\"submission.csv\", \"w\") as f:\n",
        "        f.write(\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\")\n",
        "\n",
        "        for dataset in data_dict:\n",
        "            # Only write results for datasets with images that have results\n",
        "            if dataset in results:\n",
        "                res = results[dataset]\n",
        "            else:\n",
        "                res = {}\n",
        "\n",
        "            # Same for scenes\n",
        "            for scene in data_dict[dataset]:\n",
        "                if scene in res:\n",
        "                    scene_res = res[scene]\n",
        "                else:\n",
        "                    scene_res = {\"R\":{}, \"t\":{}}\n",
        "\n",
        "                # Write the row with rotation and translation matrices\n",
        "                for image in data_dict[dataset][scene]:\n",
        "                    if image in scene_res:\n",
        "                        print(image)\n",
        "                        R = scene_res[image][\"R\"].reshape(-1)\n",
        "                        T = scene_res[image][\"t\"].reshape(-1)\n",
        "                    else:\n",
        "                        R = np.eye(3).reshape(-1)\n",
        "                        T = np.zeros((3))\n",
        "                    image_path = str(image.relative_to(base_path))\n",
        "                    f.write(f\"{image_path},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pen0JBg5BJsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "config = Config()\n",
        "data_dict = parse_sample_submission(config.base_path)\n",
        "datasets = list(data_dict.keys())\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "    if dataset not in results:\n",
        "        results[dataset] = {}\n",
        "\n",
        "    for scene in data_dict[dataset]:\n",
        "        images_dir = data_dict[dataset][scene][0].parent\n",
        "        results[dataset][scene] = {}\n",
        "        image_paths = data_dict[dataset][scene]\n",
        "        print (f\"Got {len(image_paths)} images\")\n",
        "\n",
        "        try:\n",
        "            feature_dir = config.feature_dir / f\"{dataset}_{scene}\"\n",
        "            feature_dir.mkdir(parents=True, exist_ok=True)\n",
        "            database_path = feature_dir / \"colmap.db\"\n",
        "            if database_path.exists():\n",
        "                database_path.unlink()\n",
        "\n",
        "            # 1. Get the pairs of images that are somewhat similar\n",
        "            index_pairs = get_image_pairs(\n",
        "                image_paths,\n",
        "                **config.pair_matching_args,\n",
        "                device=config.device,\n",
        "            )\n",
        "            gc.collect()\n",
        "\n",
        "            # 2. Detect keypoints of all images\n",
        "            detect_keypoints(\n",
        "                image_paths,\n",
        "                feature_dir,\n",
        "                **config.keypoint_detection_args,\n",
        "                device=config.device,\n",
        "            )\n",
        "            gc.collect()\n",
        "\n",
        "            # 3. Match  keypoints of pairs of similar images\n",
        "            keypoint_distances(\n",
        "                image_paths,\n",
        "                index_pairs,\n",
        "                feature_dir,\n",
        "                **config.keypoint_distances_args,\n",
        "                device=config.device,\n",
        "            )\n",
        "            gc.collect()\n",
        "\n",
        "            sleep(1)\n",
        "\n",
        "            # 4.1. Import keypoint distances of matches into colmap for RANSAC\n",
        "            import_into_colmap(\n",
        "                images_dir,\n",
        "                feature_dir,\n",
        "                database_path,\n",
        "            )\n",
        "\n",
        "            output_path = feature_dir / \"colmap_rec_aliked\"\n",
        "            output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # 4.2. Compute RANSAC (detect match outliers)\n",
        "            # By doing it exhaustively we guarantee we will find the best possible configuration\n",
        "            pycolmap.match_exhaustive(database_path)\n",
        "\n",
        "            mapper_options = pycolmap.IncrementalPipelineOptions(**config.colmap_mapper_options)\n",
        "\n",
        "            # 5.1 Incrementally start reconstructing the scene (sparse reconstruction)\n",
        "            # The process starts from a random pair of images and is incrementally extended by\n",
        "            # registering new images and triangulating new points.\n",
        "            maps = pycolmap.incremental_mapping(\n",
        "                database_path=database_path,\n",
        "                image_path=images_dir,\n",
        "                output_path=output_path,\n",
        "                options=mapper_options,\n",
        "            )\n",
        "\n",
        "            print(maps)\n",
        "            clear_output(wait=False)\n",
        "\n",
        "            # 5.2. Look for the best reconstruction: The incremental mapping offered by\n",
        "            # pycolmap attempts to reconstruct multiple models, we must pick the best one\n",
        "            images_registered  = 0\n",
        "            best_idx = None\n",
        "\n",
        "            print (\"Looking for the best reconstruction\")\n",
        "\n",
        "            if isinstance(maps, dict):\n",
        "                for idx1, rec in maps.items():\n",
        "                    print(idx1, rec.summary())\n",
        "                    try:\n",
        "                        if len(rec.images) > images_registered:\n",
        "                            images_registered = len(rec.images)\n",
        "                            best_idx = idx1\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "            # Parse the reconstruction object to get the rotation matrix and translation vector\n",
        "            # obtained for each image in the reconstruction\n",
        "            if best_idx is not None:\n",
        "                for k, im in maps[best_idx].images.items():\n",
        "                    key = config.base_path / \"test\" / scene / \"images\" / im.name\n",
        "                    results[dataset][scene][key] = {}\n",
        "                    results[dataset][scene][key][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n",
        "                    results[dataset][scene][key][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n",
        "\n",
        "            print(f\"Registered: {dataset} / {scene} -> {len(results[dataset][scene])} images\")\n",
        "            print(f\"Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
        "            create_submission(results, data_dict, config.base_path)\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Z0dnRAacBJsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}