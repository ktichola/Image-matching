{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 71885,
          "databundleVersionId": 8143495,
          "sourceType": "competition"
        },
        {
          "sourceId": 7884485,
          "sourceType": "datasetVersion",
          "datasetId": 4628051
        },
        {
          "sourceId": 7884725,
          "sourceType": "datasetVersion",
          "datasetId": 4628331
        },
        {
          "sourceId": 8367334,
          "sourceType": "datasetVersion",
          "datasetId": 4973903
        },
        {
          "sourceId": 8367624,
          "sourceType": "datasetVersion",
          "datasetId": 4974117
        },
        {
          "sourceId": 176805602,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 4534,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3326
        },
        {
          "sourceId": 17191,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 14317
        },
        {
          "sourceId": 17555,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 14611
        },
        {
          "sourceId": 45794,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 38394
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "📸 IMC - with rotation on match pair",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktichola/Image-matching/blob/main/%F0%9F%93%B8_IMC_with_rotation_on_match_pair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'image-matching-challenge-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F71885%2F8143495%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085953Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1eeb2152d1864b22c560089328e39b04229145160c8a79967bb96266396882810e9532d6d8b78404a09197a6fec7313d794bf594a18dd61fe8219cc4f06cd54dff5b40683c098738b81aeb75af7459fe37966dcb55a87199c13e3f3823df663338a430c4c72cb265b852f56486a405243b8865ccbaf47d70de688eaf06e512e6d688b4808a4c99e7190298723dd9f6e3e0a94989ca6285931f0360391291cd1d5c81c75ef1b5bee3fd1f63babb1e6e1e6a7c5b84028133040207e06b32c5e3841d238d28b3d0330b036aaf27770ac7512119f70464dd46c1b83012787999c3dc792c50eb37c356cb7e090ca9c5a9701b85ee8b1b0c99272d5c8e402bd5ad0fa9,imc2024-packages-lightglue-rerun-kornia:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4628051%2F7884485%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085953Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1147bd3b455764b305c9c9a88ff5106210f623a632a9cb480a920f67fa2574038f5063370ea4382d55c4ee92580f0baeefa6c94e69db53efafa42fb275bd1aae2d9642c021436cf26033c8f97a61b3aa979a007c08f765206c3cded532293a29964e9d4dd11a9df7f47e77d9d72c78a8ea644fb53b438991bb13b65cd23b92710c502ddc9515a669995453157a6f53dfc5ef13af10412a57e0628675dec144009ce9747ca8e18e06d33b0241c6e384a09ed16ab0986fe37a17c93d781a907f897c4b9db3d693e9bd7daeb0baafa06b624cabff9ec4b980c420f87509700deb08a17c44f443f887604b8775b2da87c5555eb263a0e6c498418fe2ca5db1f38b88,colmap-db-import:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4628331%2F7884725%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2637b550f9e3d0df2f4a953a16e04fa7a5ca1ac280e00bf0fc6c07953193ab9a4ba3dd9ad70eac0901208c8d20f03dd9bae42bac219b358506daec6c5ce6e20d002c1ea957382ccb54ec5a0b0d154f7d10ba01010efc83cb93f7ed05f115b7572770173ce31cfa84c4a7a38b0448c2ba287e275d98f2416a82e64056991f1eb7ba2abed49d82fb0003bcc864cb611fb02b81bbd848215bd9c1908b6fd272672fb9d5b284adc9caf6ef4a1b13b1439887c2ca2c4cbc40d4bab8a27d66efa7c260d406550880e9c5869843684277b7f7c421adff2cfd82556306527a78de85ea445e1063c93ff77c80118bc3332fa5a946f41b971aef2af8f816d4ee4979cc2a0c,mediapy:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4973903%2F8367334%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7e98eaeb5595a02881b4ee24d7ca023b7d29623e5fc400c92afec0922478b5a96c843a4e4712683bcfce8030656f7d0e051558c792641782ee5dc29262dfce121b40c9194ae5d3cd54d691d7ba6d5a77362a86a5260773c95f29f514bcb86f6214b4fadd78f8202b8b5377cbb3b81f9e002f51dc25ccfce25cbcc4776742603add8c2f310cc9a256a20cbfb589b948aebb2f6ec7e0ea73c6669b4c44c3cfc9d6f00e87f9c999921fcc9129ca90e8b1f99ef5acfb4e4a9a875fb711b2551c48c68e2e3ba94da94b4eaf1d5389223eadff4057ed99836d549c3c09849cf31c934a4acfc0e87c2677714d16db9421227164a07609d2bfcf4798f8ffd0fb168d6ae6,check-orientation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4974117%2F8367624%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da4a89e2f5e46e3e8a85b663489d0c441683a86c5a8cebc0fc5d27456de92b055bc831382422fcae5805c33c32d880388af811333f8ef0db26b604ea51fea43831d8f40155f2abee81672f594e3e085320ecf1c9d1f4795bccbf37178fe93091fb157e48f3150578bba4c50c80e1e6cd11d50acb2099fe357f0711cf1fc83cd5be07f81e0712e1f7151978c761f82ab3e4fbfe20eae60e92c0e7d18b19a950bc44978e0039847346b3d96dd0e31a94f1d2ca6195371e6e654467312345bc6848bdf10d580b17101c2c5628c62ac950ee98cba318f959243adc0fcec6a0b997b2411e8c4f4d8ef2588e1fff2c0b5b43ba60e94861a523ca2da475c4040bc808dad,dinov2/pytorch/base/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3326%2F4534%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9a05c6363a548072010f0c6f327b777339084c17d76e5ca3e4432522e83888ec23f9adc8449212d2dc4014dacf0aadf9bbe047d16d2c334a348ffa7ee5e078a7a6b38d7718b06b2f82d506381cc7085a9efafa8d1275696767bad16a7c8c594952f7983ca7329537b94cac6c51059c31cc0330e306d22c57163f331574118b9ef41878e8e41695e6718904a2417f3f6ca35c56247c0b881995755e081abb744e7cd87487bc54ae0489017085a334c42bdb48c9bd0c916610fb574f2d650ced93fcd041b5a71eb6502eb7356581bb7491a67e1b72459557d50221d7f19fd3daf0e6a3b651d6847bbf121aefdc23a1a4aab6e5ceccdf87016e8d2248b9a4b2a9d2,lightglue/pytorch/aliked/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F14317%2F17191%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9eaedec1dcc70334cb71c46049d9888f7e5c9a19697c48e2f04245b387ca9caba90e0dd5e12741cf83dd0b6dabac7b18d05be6252a4fc33adbcdee08d11eb7cd7304996b2b193a3d1e69c34a91344903fcf3853f73faed2c6acbc639c6faf42e7232d2f6514bc3eeee775dca627128e3ef3cecc2658f3ea4440cc1936bba21642c84c2cc3ee222804b40b5160cd950ee7012f181558feb57669d13fc75ba50e8aeadbafa50d2f5401d58c9c5aa7ccbd8f5d45ef64e04f2608813aeb7f37a2a392984ba4791d75434bee3f8e4b4e761a3e17336c35aed929486e88052edf62cb92956e71c304c20b8a04f6bcc7d5302048706dfd6cf0c7e29c665289fce02bb13,aliked/pytorch/aliked-n16/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F14611%2F17555%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4e6964051ba2706cc5b8f9f0d9efa90f2d7783a7b31b6a894de4c69bee69fbf3b6b38daf77c6f4eeff48f3f65d53336cae3de61e52156a1642b8c7f8ab0e948a5399b93f21eeb19e75a60c9dacd6744ef2ef695b813992ababb97b67957d0b9839389523b32838d5c7b2b966a2e0069341811f8dcf30043004477b3bc191d418c96887ff5e20de0a583966a98513316f4562a9cbc5c8c9ca2a6db14747bd9c89361bb1f76efb6ab25efa43223e931d26da258b276f3105e5d907c0899266a8b6a742092e4d5667e4765f870c0e056db6b0f9ae821dbb866a632f02874c7fe1e4bc3489752f2c73a390fcf91c6fe75fa7f326cec42c81e842e9db44e00955ab55,swsl_resnext50_32x4d/pytorch/v1/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F38394%2F45794%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240515%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240515T085954Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9d959d27493b3b9d0e061f1381a501a7a66c195ce37fd432ea3aa4a6c0825a1439bea262fa75ac33b612831507aabbfc5e828f5dd59c8cacad3d819afe29d9757f9fbcc23852e0a9c64ff8a42303213a6f119af124dbd94cf8789a6522715fea5d09fbbe8bf74777a89ab2478e4e62d3ec655954446ad9594ea51845181f0633e8e5e50d1e92785a48e45acc05488d9d6e184c29f225ee113009fea2ca18e47da278aa1a8600f249cc900be8d9f235603d569c6a4328e104cc97030b83460db4e2216f44a0fb6317477d7095c6f71dd09126d5c15bbb83cfd2ad7fd2d102b3f383a6341a133ac36e2552ea6ff805c490f991b12b4d6f2525a745d20233e88262'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "WkYFH-RGetX_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\"> 📸 Image Matching Challenge - 📊 Understanding the baseline</center>\n",
        "<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">Reconstruct 3D scenes from 2D images over six different domains</center></p>\n",
        "\n",
        "***\n",
        "\n",
        "In this notebook I explain the baseline solution provided by the organizers in [this notebook](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
        "\n",
        "I have made the code a bit easier to read, adding comments and type annotations to make it easier for you to understand what is going on.\n",
        "\n",
        "Hope you enjoy ❤️"
      ],
      "metadata": {
        "id": "_GIqQRQWetYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure from Motion"
      ],
      "metadata": {
        "id": "7btpxzcTetYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structure from Motion (SfM) is the name given to the procedure of **reconstructing a 3D scene and simultaneously obtaining the camera poses of a camera w.r.t. the given scene**. This means that, as the name suggests, we are creating the entire rigid structure from a set of images with different view points (or equivalently a camera in motion).\n",
        "\n",
        "In this competition, the important aspect of SfM we are interested in is *obtaining the camera poses* of where each image was taken, described by a rotation matrix and translation vector from the origin. These are the objects that will be scored in our submission!"
      ],
      "metadata": {
        "id": "IeNA6bueetYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.ENP48SmZHwG3r3O0lUVcWAHaFf%26pid%3DApi&f=1&ipt=550bf79efa85e7af870dd2d0a16793af7f3f83a36c14a1f4a648659a384b5a98&ipo=images\" alt=\"Structure from motion structure: multiple cameras pointing toward an object in different positions and rotations that we need to find.\"></center>"
      ],
      "metadata": {
        "id": "i-BbPWFZetYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline solution steps\n",
        "In order to be able to estimate the camera poses, the solution provided by the organizers consists in the following steps:\n",
        "\n",
        "* [1. Find pairs of images that are similar](#1)\n",
        "* [2. Compute image keypoints](#2)\n",
        "* [3. Match keypoints between images](#3)\n",
        "* [4. Outlier detection with RANSAC](#4)\n",
        "* [5. Sparse reconstruction](#5)\n",
        "\n",
        "Let's understand how these steps are carried out"
      ],
      "metadata": {
        "id": "F2nRcOUPetYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing & importing relevant packages and models"
      ],
      "metadata": {
        "id": "DQJjJBGFetYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
        "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
        "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
        "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
        "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
        "\n",
        "##############ADDED######################\n",
        "!pip install --no-index --find-links=/kaggle/input/mediapy/mediapy mediapy\n",
        "!pip install --no-index /kaggle/input/check-orientation/check-orientation/iglovikov_helper_functions-0.0.53-py2.py3-none-any.whl --no-deps\n",
        "#######################################"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-05-15T08:52:32.644855Z",
          "iopub.execute_input": "2024-05-15T08:52:32.645501Z",
          "iopub.status.idle": "2024-05-15T08:52:57.480843Z",
          "shell.execute_reply.started": "2024-05-15T08:52:32.645472Z",
          "shell.execute_reply": "2024-05-15T08:52:57.479875Z"
        },
        "trusted": true,
        "id": "AKDl_gDYetYH",
        "outputId": "ef5815ef-9cca-48aa-d0f4-d3574f7494d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nInstalling collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\n  Attempting uninstall: kornia\n    Found existing installation: kornia 0.7.1\n    Uninstalling kornia-0.7.1:\n      Successfully uninstalled kornia-0.7.1\nSuccessfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\nLooking in links: /kaggle/input/mediapy/mediapy\nProcessing /kaggle/input/mediapy/mediapy/mediapy-1.2.0-py3-none-any.whl\nRequirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from mediapy) (8.20.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapy) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mediapy) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from mediapy) (9.5.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (5.9.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->mediapy) (4.8.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapy) (2.8.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->mediapy) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->mediapy) (0.2.13)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.16.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->mediapy) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->mediapy) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->mediapy) (0.2.2)\nInstalling collected packages: mediapy\nSuccessfully installed mediapy-1.2.0\nProcessing /kaggle/input/check-orientation/check-orientation/iglovikov_helper_functions-0.0.53-py2.py3-none-any.whl\nInstalling collected packages: iglovikov-helper-functions\nSuccessfully installed iglovikov-helper-functions-0.0.53\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General utilities\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from time import time, sleep\n",
        "from fastprogress import progress_bar\n",
        "import gc\n",
        "import numpy as np\n",
        "import h5py\n",
        "from IPython.display import clear_output\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "from typing import Any\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "#########ADDED###########\n",
        "import mediapy as media\n",
        "import cv2\n",
        "from glob import glob\n",
        "from pprint import pprint\n",
        "import shutil\n",
        "\n",
        "from collections import namedtuple\n",
        "from typing import Optional\n",
        "from iglovikov_helper_functions.dl.pytorch.utils import rename_layers\n",
        "from timm import create_model as timm_create_model\n",
        "from torch import nn\n",
        "###################\n",
        "\n",
        "# CV/MLe\n",
        "import cv2\n",
        "import torch\n",
        "from torch import Tensor as T\n",
        "import torch.nn.functional as F\n",
        "import kornia as K\n",
        "import kornia.feature as KF\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "import torch\n",
        "from lightglue import match_pair\n",
        "from lightglue import LightGlue, ALIKED\n",
        "from lightglue.utils import load_image, rbd\n",
        "\n",
        "# 3D reconstruction\n",
        "import pycolmap\n",
        "\n",
        "# Data importing into colmap\n",
        "import sys\n",
        "sys.path.append(\"/kaggle/input/colmap-db-import\")\n",
        "\n",
        "# Provided by organizers\n",
        "from database import *\n",
        "from h5_to_db import *\n",
        "\n",
        "def arr_to_str(a):\n",
        "    \"\"\"Returns ;-separated string representing the input\"\"\"\n",
        "    return \";\".join([str(x) for x in a.reshape(-1)])\n",
        "\n",
        "def load_torch_image(file_name: Path | str, device=torch.device(\"cpu\")):\n",
        "    \"\"\"Loads an image and adds batch dimension\"\"\"\n",
        "    img = K.io.load_image(file_name, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
        "    return img\n",
        "\n",
        "device = K.utils.get_cuda_device_if_available(0)\n",
        "print(device)\n",
        "\n",
        "DEBUG = len([p for p in Path(\"/kaggle/input/image-matching-challenge-2024/test/\").iterdir() if p.is_dir()]) == 2\n",
        "DEBUG = False\n",
        "print(\"DEBUG:\", DEBUG)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-05-15T08:52:57.482743Z",
          "iopub.execute_input": "2024-05-15T08:52:57.483053Z",
          "iopub.status.idle": "2024-05-15T08:53:16.276298Z",
          "shell.execute_reply.started": "2024-05-15T08:52:57.483025Z",
          "shell.execute_reply": "2024-05-15T08:53:16.275336Z"
        },
        "trusted": true,
        "id": "J7DbTitEetYI",
        "outputId": "c02fe7ac-dd8d-4aad-8c02-a8ce4f6d72ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-05-15 08:53:08.477014: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-15 08:53:08.477113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-15 08:53:08.607147: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "cuda:0\nDEBUG: False\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JEANS = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:16.27739Z",
          "iopub.execute_input": "2024-05-15T08:53:16.278029Z",
          "iopub.status.idle": "2024-05-15T08:53:16.281908Z",
          "shell.execute_reply.started": "2024-05-15T08:53:16.278001Z",
          "shell.execute_reply": "2024-05-15T08:53:16.281069Z"
        },
        "trusted": true,
        "id": "FfN8_3foetYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Check Orientation model\n",
        "Input an image\n",
        "\n",
        "Output a prediction in 4 rotation class [0deg 90deg 180deg 270deg]\n",
        "\n",
        "using the MODELS at /kaggle/input/swsl_resnext50_32x4d/pytorch/v1/1"
      ],
      "metadata": {
        "id": "tMvaekp4etYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_name: str, activation: Optional[str] = \"softmax\") -> nn.Module:\n",
        "    # Define the 'model' namedtuple\n",
        "    model = namedtuple(\"model\", [\"url\", \"model\"])\n",
        "\n",
        "    # Define the models dictionary\n",
        "    models = {\n",
        "        \"swsl_resnext50_32x4d\": model(\n",
        "            model=timm_create_model(\"swsl_resnext50_32x4d\", pretrained=False, num_classes=4),\n",
        "            url=\"/kaggle/input/swsl_resnext50_32x4d/pytorch/v1/1/2020-11-16_resnext50_32x4d.pth\",\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # Load the model from models dictionary\n",
        "    model = models[model_name].model\n",
        "\n",
        "    # Load the model state dictionary from local path\n",
        "    state_dict = torch.load(models[model_name].url, map_location=\"cpu\")[\"state_dict\"]\n",
        "    state_dict = rename_layers(state_dict, {\"model.\": \"\"})\n",
        "    # Load the state dictionary into the model\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    if activation == \"softmax\":\n",
        "        return nn.Sequential(model, nn.Softmax(dim=1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:16.28425Z",
          "iopub.execute_input": "2024-05-15T08:53:16.28453Z",
          "iopub.status.idle": "2024-05-15T08:53:16.606305Z",
          "shell.execute_reply.started": "2024-05-15T08:53:16.284507Z",
          "shell.execute_reply": "2024-05-15T08:53:16.605264Z"
        },
        "trusted": true,
        "id": "QyOtA64OetYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orientation_model = create_model(\"swsl_resnext50_32x4d\")\n",
        "orientation_model.eval()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:16.60746Z",
          "iopub.execute_input": "2024-05-15T08:53:16.607745Z",
          "iopub.status.idle": "2024-05-15T08:53:17.924091Z",
          "shell.execute_reply.started": "2024-05-15T08:53:16.607722Z",
          "shell.execute_reply": "2024-05-15T08:53:17.923134Z"
        },
        "trusted": true,
        "id": "3dRbbIPHetYJ",
        "outputId": "646014c4-122b-4a6b-9bdd-cd87e13ab9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnext50_32x4d to current resnext50_32x4d.fb_swsl_ig1b_ft_in1k.\n  model = create_fn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Sequential(\n  (0): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (fc): Linear(in_features=2048, out_features=4, bias=True)\n  )\n  (1): Softmax(dim=1)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image(image, degrees):\n",
        "    \"\"\"\n",
        "    Rotate a Torch tensor representing an image by a specified angle along dimensions 2 and 3.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Input image tensor.\n",
        "        degrees (int): Rotation angle in degrees (90, 180, or 270).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Rotated image tensor.\n",
        "    \"\"\"\n",
        "    if degrees not in [90, 180, 270]:\n",
        "        raise ValueError(\"Rotation angle must be 90, 180, or 270 degrees.\")\n",
        "\n",
        "    # Calculate the number of times to rotate the tensor by 90 degrees\n",
        "    k = degrees // 90\n",
        "\n",
        "    # Rotate the image tensor along dimensions 2 and 3\n",
        "    rotated_image = torch.rot90(image, k=k, dims=(2, 3))\n",
        "\n",
        "    return rotated_image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:17.92553Z",
          "iopub.execute_input": "2024-05-15T08:53:17.926025Z",
          "iopub.status.idle": "2024-05-15T08:53:17.931672Z",
          "shell.execute_reply.started": "2024-05-15T08:53:17.925992Z",
          "shell.execute_reply": "2024-05-15T08:53:17.930625Z"
        },
        "trusted": true,
        "id": "jSlivDLRetYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_orientation(image,path):\n",
        "\n",
        "        if DEBUG:\n",
        "            plt.imshow(image.squeeze(0).permute(1, 2, 0).numpy())\n",
        "            plt.axis('off')  # Hide axis\n",
        "            plt.show()\n",
        "\n",
        "        resized_image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "        ####################\n",
        "        with torch.no_grad():\n",
        "            prediction = orientation_model(resized_image).numpy()[0]\n",
        "\n",
        "        predictedRotationDeg = 360 - (prediction.argmax() * 90)\n",
        "\n",
        "        if predictedRotationDeg == 360:\n",
        "            print(f'{path}`s orientation is correct.')\n",
        "        else:\n",
        "            print(f'{path}`s ROTATED.')\n",
        "            image = rotate_image(image, predictedRotationDeg)\n",
        "        ####################\n",
        "\n",
        "        if DEBUG:\n",
        "            plt.imshow(image.squeeze(0).permute(1, 2, 0).numpy())\n",
        "            plt.axis('off')  # Hide axis\n",
        "            plt.show()\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:17.932965Z",
          "iopub.execute_input": "2024-05-15T08:53:17.933562Z",
          "iopub.status.idle": "2024-05-15T08:53:17.943816Z",
          "shell.execute_reply.started": "2024-05-15T08:53:17.933537Z",
          "shell.execute_reply": "2024-05-15T08:53:17.942877Z"
        },
        "trusted": true,
        "id": "o-onaFp9etYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotation Correction\n",
        "In this section, we correct the rotation of the images."
      ],
      "metadata": {
        "id": "JYKnlkU4etYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# Finding image pairs\n",
        "\n",
        "To find pairs of similar images, we use [DINOv2](https://arxiv.org/pdf/2304.07193.pdf) to obtain normalized image embeddings.\n",
        "\n",
        "<center><img src=\"https://www.labellerr.com/blog/content/images/2023/05/Dino-v2-20230419.jpg\" alt=\"DINOv2 example\"></center>\n",
        "Then, we calculate the distances between all the embeddings, and only keep those below a given distance threshold. For images with less than a set minimum number of pairs, the closest ones are kept instead."
      ],
      "metadata": {
        "id": "v8rrn71cetYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_images(\n",
        "    paths: list[Path],\n",
        "    model_name: str,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") -> T:\n",
        "    \"\"\"Computes image embeddings.\n",
        "\n",
        "    Returns a tensor of shape [len(filenames), output_dim]\n",
        "    \"\"\"\n",
        "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).eval().to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for i, path in tqdm(enumerate(paths), desc=\"Global descriptors\"):\n",
        "        image = load_torch_image(path)\n",
        "        print(image.shape)\n",
        "        ########################################################################################################################\n",
        "        image = correct_orientation(image,path)\n",
        "        ########################################################################################################################\n",
        "        with torch.inference_mode():\n",
        "            inputs = processor(images=image, return_tensors=\"pt\", do_rescale=False).to(device)\n",
        "            outputs = model(**inputs) # last_hidden_state and pooled\n",
        "\n",
        "            # Max pooling over all the hidden states but the first (starting token)\n",
        "            # To obtain a tensor of shape [1, output_dim]\n",
        "            # We normalize so that distances are computed in a better fashion later\n",
        "            embedding = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=-1, p=2)\n",
        "\n",
        "        embeddings.append(embedding.detach().cpu())\n",
        "    return torch.cat(embeddings, dim=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:17.944727Z",
          "iopub.execute_input": "2024-05-15T08:53:17.944981Z",
          "iopub.status.idle": "2024-05-15T08:53:17.958642Z",
          "shell.execute_reply.started": "2024-05-15T08:53:17.944959Z",
          "shell.execute_reply": "2024-05-15T08:53:17.957835Z"
        },
        "trusted": true,
        "id": "hS8bmPpNetYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs_exhaustive(lst: list[Any]) -> list[tuple[int, int]]:\n",
        "    \"\"\"Obtains all possible index pairs of a list\"\"\"\n",
        "    return list(itertools.combinations(range(len(lst)), 2))\n",
        "\n",
        "def get_image_pairs(\n",
        "    paths: list[Path],\n",
        "    model_name: str,\n",
        "    similarity_threshold: float = 0.6,\n",
        "    tolerance: int = 1000,\n",
        "    min_matches: int = 20,\n",
        "    exhaustive_if_less: int = 20,\n",
        "    p: float = 2.0,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") -> list[tuple[int, int]]:\n",
        "    \"\"\"Obtains pairs of similar images\"\"\"\n",
        "    if len(paths) <= exhaustive_if_less:\n",
        "        print(\"Do exhaustive pair\")\n",
        "        return get_pairs_exhaustive(paths)\n",
        "\n",
        "    print(f'Use image embed dist pair with similarity_threshold = {similarity_threshold}')\n",
        "    matches = []\n",
        "\n",
        "    # Embed images and compute distances for filtering\n",
        "    embeddings = embed_images(paths, model_name)\n",
        "    distances = torch.cdist(embeddings, embeddings, p=p)\n",
        "\n",
        "    # Remove pairs above similarity threshold (if enough)\n",
        "    mask = distances <= similarity_threshold\n",
        "    image_indices = np.arange(len(paths))\n",
        "\n",
        "    for current_image_index in range(len(paths)):\n",
        "        mask_row = mask[current_image_index]\n",
        "        indices_to_match = image_indices[mask_row]\n",
        "\n",
        "        # We don't have enough matches below the threshold, we pick most similar ones\n",
        "        if len(indices_to_match) < min_matches:\n",
        "            indices_to_match = np.argsort(distances[current_image_index])[:min_matches]\n",
        "\n",
        "        for other_image_index in indices_to_match:\n",
        "            # Skip an image matching itself\n",
        "            if other_image_index == current_image_index:\n",
        "                continue\n",
        "\n",
        "            # We need to check if we are below a certain distance tolerance\n",
        "            # since for images that don't have enough matches, we picked\n",
        "            # the most similar ones (which could all still be very different\n",
        "            # to the image we are analyzing)\n",
        "            if distances[current_image_index, other_image_index] < tolerance:\n",
        "                # Add the pair in a sorted manner to avoid redundancy\n",
        "                matches.append(tuple(sorted((current_image_index, other_image_index.item()))))\n",
        "\n",
        "    return sorted(list(set(matches)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:17.959833Z",
          "iopub.execute_input": "2024-05-15T08:53:17.960111Z",
          "iopub.status.idle": "2024-05-15T08:53:17.974502Z",
          "shell.execute_reply.started": "2024-05-15T08:53:17.960089Z",
          "shell.execute_reply": "2024-05-15T08:53:17.973616Z"
        },
        "trusted": true,
        "id": "2QI4OsxYetYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if JEANS:\n",
        "#     images_list = list(Path(\"/kaggle/input/image-matching-challenge-2024/test/church/images/\").glob(\"*.png\"))[:30]\n",
        "#     index_pairs = get_image_pairs(images_list, \"/kaggle/input/dinov2/pytorch/base/1\")\n",
        "#     print(index_pairs)\n",
        "\n",
        "if DEBUG:\n",
        "    images_list = list(Path(\"/kaggle/input/image-matching-challenge-2024/train/dioscuri/images\").glob(\"*.png\"))[:30]\n",
        "    index_pairs = get_image_pairs(images_list, \"/kaggle/input/dinov2/pytorch/base/1\")\n",
        "    print(index_pairs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:17.977805Z",
          "iopub.execute_input": "2024-05-15T08:53:17.978483Z",
          "iopub.status.idle": "2024-05-15T08:53:17.987747Z",
          "shell.execute_reply.started": "2024-05-15T08:53:17.978452Z",
          "shell.execute_reply": "2024-05-15T08:53:17.98685Z"
        },
        "trusted": true,
        "id": "9Ip-BNJYetYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2\"></a>\n",
        "# Computing keypoints\n",
        "\n",
        "In order to be able to know the position of each camera, we must be able to relate images to each other. For this, we extract relevant keypoints and compare pairs of image keypoints against each other. There are many ways to extract relevant keypoints, the most traditional one being [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). However, newer and improved methods exist now, one of which is [ALIKED](https://arxiv.org/abs/2304.03608), the keypoint extraction method used in the solution.\n",
        "\n",
        "<center><img src=\"https://www.catalyzex.com/_next/image?url=https%3A%2F%2Fd3i71xaburhd42.cloudfront.net%2Faf9fc17471b4c38211c3d9f5058c9c1f59501eea%2F3-Figure1-1.png&w=640&q=75\" alt=\"ALIKED architecture\"></center>\n",
        "\n",
        "\n",
        "Let's take a closer look at the keypoints that ALIKED extracts."
      ],
      "metadata": {
        "id": "HQxXvkIOetYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    dtype = torch.float32 # ALIKED has issues with float16\n",
        "\n",
        "    extractor = ALIKED(\n",
        "            max_num_keypoints=4096,\n",
        "            detection_threshold=0.01,\n",
        "            resize=1024\n",
        "        ).eval().to(device, dtype)\n",
        "\n",
        "    path = images_list[0]\n",
        "    image = load_torch_image(path, device=device).to(dtype)\n",
        "    features = extractor.extract(image)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 20))\n",
        "    ax[0].imshow(image[0, ...].permute(1,2,0).cpu())\n",
        "    ax[1].imshow(image[0, ...].permute(1,2,0).cpu())\n",
        "    ax[1].scatter(features[\"keypoints\"][0, :, 0].cpu(), features[\"keypoints\"][0, :, 1].cpu(), s=0.5, c=\"red\")\n",
        "\n",
        "    del extractor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:17.988774Z",
          "iopub.execute_input": "2024-05-15T08:53:17.989067Z",
          "iopub.status.idle": "2024-05-15T08:53:18.001203Z",
          "shell.execute_reply.started": "2024-05-15T08:53:17.989044Z",
          "shell.execute_reply": "2024-05-15T08:53:18.000309Z"
        },
        "trusted": true,
        "id": "gDzpot6qetYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_keypoints(\n",
        "    paths: list[Path],\n",
        "    feature_dir: Path,\n",
        "    num_features: int = 4096,\n",
        "    resize_to: int = 1024,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") -> None:\n",
        "    \"\"\"Detects the keypoints in a list of images with ALIKED\n",
        "\n",
        "    Stores them in feature_dir/keypoints.h5 and feature_dir/descriptors.h5\n",
        "    to be used later with LightGlue\n",
        "    \"\"\"\n",
        "    dtype = torch.float32 # ALIKED has issues with float16\n",
        "\n",
        "    extractor = ALIKED(\n",
        "        max_num_keypoints=num_features,\n",
        "        detection_threshold=0.01,\n",
        "        resize=resize_to\n",
        "    ).eval().to(device, dtype)\n",
        "\n",
        "    feature_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"w\") as f_keypoints, \\\n",
        "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"w\") as f_descriptors:\n",
        "\n",
        "        for path in tqdm(paths, desc=\"Computing keypoints\"):\n",
        "            key = path.name\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                image = load_torch_image(path, device=device).to(dtype)\n",
        "                features = extractor.extract(image)\n",
        "\n",
        "                f_keypoints[key] = features[\"keypoints\"].squeeze().detach().cpu().numpy()\n",
        "                f_descriptors[key] = features[\"descriptors\"].squeeze().detach().cpu().numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.002327Z",
          "iopub.execute_input": "2024-05-15T08:53:18.002738Z",
          "iopub.status.idle": "2024-05-15T08:53:18.011919Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.002709Z",
          "shell.execute_reply": "2024-05-15T08:53:18.011162Z"
        },
        "trusted": true,
        "id": "qD1prlGeetYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    feature_dir = Path(\"./sample_test_features\")\n",
        "    detect_keypoints(images_list, feature_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.012845Z",
          "iopub.execute_input": "2024-05-15T08:53:18.013066Z",
          "iopub.status.idle": "2024-05-15T08:53:18.025289Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.013047Z",
          "shell.execute_reply": "2024-05-15T08:53:18.024587Z"
        },
        "trusted": true,
        "id": "J3pTniNEetYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3\"></a>\n",
        "# Match and compute keypoint distances\n",
        "\n",
        "Now that we have the relevant image pairs and keypoints, we can go ahead and compare the keypoints of the images in a pair to find a good relationship between them. This is done with [LightGlue](https://arxiv.org/abs/2306.13643), which matches the keypoints and their descriptors between two images.\n",
        "\n",
        "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fraw.githubusercontent.com%2Fcvg%2Flightglue%2Fmaster%2Fassets%2Feasy_hard.jpg&f=1&nofb=1&ipt=60962b56b05d3e8f95a064ab2a6010e5a6cbd5f1d10379d90e660b2561a3bae9&ipo=images\" alt=\"LightGlue example\"></center>"
      ],
      "metadata": {
        "id": "247Y2pWbetYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    matcher_params = {\n",
        "        \"width_confidence\": -1,\n",
        "        \"depth_confidence\": -1,\n",
        "        \"mp\": True if 'cuda' in str(device) else False,\n",
        "    }\n",
        "    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n",
        "\n",
        "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n",
        "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors:\n",
        "            idx1, idx2 = index_pairs[0]\n",
        "            key1, key2 = images_list[idx1].name, images_list[idx2].name\n",
        "\n",
        "            keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n",
        "            keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n",
        "            print(\"Keypoints:\", keypoints1.shape, keypoints2.shape)\n",
        "            descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n",
        "            descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n",
        "            print(\"Descriptors:\", descriptors1.shape, descriptors2.shape)\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                distances, indices = matcher(\n",
        "                    descriptors1,\n",
        "                    descriptors2,\n",
        "                    KF.laf_from_center_scale_ori(keypoints1[None]),\n",
        "                    KF.laf_from_center_scale_ori(keypoints2[None]),\n",
        "                )\n",
        "    print(distances, indices)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.026525Z",
          "iopub.execute_input": "2024-05-15T08:53:18.026778Z",
          "iopub.status.idle": "2024-05-15T08:53:18.036519Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.026757Z",
          "shell.execute_reply": "2024-05-15T08:53:18.035758Z"
        },
        "trusted": true,
        "id": "fqmn8WDIetYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keypoint_distances(\n",
        "    paths: list[Path],\n",
        "    index_pairs: list[tuple[int, int]],\n",
        "    feature_dir: Path,\n",
        "    min_matches: int = 15,\n",
        "    verbose: bool = True,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        ") -> None:\n",
        "    \"\"\"Computes distances between keypoints of images.\n",
        "\n",
        "    Stores output at feature_dir/matches.h5\n",
        "    \"\"\"\n",
        "\n",
        "    matcher_params = {\n",
        "        \"width_confidence\": -1,\n",
        "        \"depth_confidence\": -1,\n",
        "        \"mp\": True if 'cuda' in str(device) else False,\n",
        "    }\n",
        "    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n",
        "\n",
        "    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n",
        "         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors, \\\n",
        "         h5py.File(feature_dir / \"matches.h5\", mode=\"w\") as f_matches:\n",
        "\n",
        "            for idx1, idx2 in tqdm(index_pairs, desc=\"Computing keypoing distances\"):\n",
        "                key1, key2 = paths[idx1].name, paths[idx2].name\n",
        "\n",
        "                keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n",
        "                keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n",
        "                descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n",
        "                descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n",
        "\n",
        "                with torch.inference_mode():\n",
        "                    distances, indices = matcher(\n",
        "                        descriptors1,\n",
        "                        descriptors2,\n",
        "                        KF.laf_from_center_scale_ori(keypoints1[None]),\n",
        "                        KF.laf_from_center_scale_ori(keypoints2[None]),\n",
        "                    )\n",
        "\n",
        "                # We have matches to consider\n",
        "                n_matches = len(indices)\n",
        "                if n_matches:\n",
        "                    if verbose:\n",
        "                        print(f\"{key1}-{key2}: {n_matches} matches\")\n",
        "                    # Store the matches in the group of one image\n",
        "                    if n_matches >= min_matches:\n",
        "                        group  = f_matches.require_group(key1)\n",
        "                        group.create_dataset(key2, data=indices.detach().cpu().numpy().reshape(-1, 2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.037623Z",
          "iopub.execute_input": "2024-05-15T08:53:18.037921Z",
          "iopub.status.idle": "2024-05-15T08:53:18.051207Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.037893Z",
          "shell.execute_reply": "2024-05-15T08:53:18.050381Z"
        },
        "trusted": true,
        "id": "G9wtMKVsetYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    keypoint_distances(images_list, index_pairs, feature_dir, verbose=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.052307Z",
          "iopub.execute_input": "2024-05-15T08:53:18.052654Z",
          "iopub.status.idle": "2024-05-15T08:53:18.064758Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.052623Z",
          "shell.execute_reply": "2024-05-15T08:53:18.063974Z"
        },
        "trusted": true,
        "id": "I4lOSXnLetYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4\"></a>\n",
        "# RANSAC\n",
        "Up to now, we have matched keypoints and their descriptors extracted from pairs of images. This is described by a [fundamental matrix](https://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)) denoted as $F$. In epipolar geometry, with homogeneous image coordinates, $x$ and $x′$, of corresponding points in a stereo image pair, $Fx$ describes a line (an epipolar line) on which the corresponding point $x′$ on the other image must lie. That means, for all pairs of corresponding points, $x'Fx = 0$ holds. This is known as epipolar constraint or correspondance condition (or Longuet-Higgins equation), and is solved via the [eight-point algorithm](https://en.wikipedia.org/wiki/Eight-point_algorithm).\n",
        "\n",
        "<center><img src=\"https://cmsc426.github.io/assets/sfm/epipole1.png\" alt=\"Fundamental matrix\"></center>\n",
        "\n",
        "Since the keypoint correspondences are computed using feature descriptors, the data is bound to be noisy and (in general) contains several outliers. Thus, to remove these outliers, we use a [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus) algorithm to find the best possible fundamental matrix. So, out of all possibilities, the $F$ matrix with maximum number of inliers is chosen.\n",
        "\n",
        "<center><img src=\"https://cmsc426.github.io/assets/sfm/ransac.png\" alt=\"RANSAC\"></center>"
      ],
      "metadata": {
        "id": "cpSuP6z1etYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def import_into_colmap(\n",
        "    path: Path,\n",
        "    feature_dir: Path,\n",
        "    database_path: str = \"colmap.db\",\n",
        ") -> None:\n",
        "    \"\"\"Adds keypoints into colmap\"\"\"\n",
        "    db = COLMAPDatabase.connect(database_path)\n",
        "    db.create_tables()\n",
        "    single_camera = False\n",
        "    fname_to_id = add_keypoints(db, feature_dir, path, \"\", \"simple-pinhole\", single_camera)\n",
        "    add_matches(\n",
        "        db,\n",
        "        feature_dir,\n",
        "        fname_to_id,\n",
        "    )\n",
        "    db.commit()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.065722Z",
          "iopub.execute_input": "2024-05-15T08:53:18.066092Z",
          "iopub.status.idle": "2024-05-15T08:53:18.074578Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.066066Z",
          "shell.execute_reply": "2024-05-15T08:53:18.073787Z"
        },
        "trusted": true,
        "id": "K_1rRzU7etYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    database_path = \"colmap.db\"\n",
        "    images_dir = images_list[0].parent\n",
        "    import_into_colmap(\n",
        "        images_dir,\n",
        "        feature_dir,\n",
        "        database_path,\n",
        "    )\n",
        "\n",
        "    # This does RANSAC\n",
        "    pycolmap.match_exhaustive(database_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.075766Z",
          "iopub.execute_input": "2024-05-15T08:53:18.076015Z",
          "iopub.status.idle": "2024-05-15T08:53:18.088416Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.075994Z",
          "shell.execute_reply": "2024-05-15T08:53:18.087571Z"
        },
        "trusted": true,
        "id": "Lr2_SdcPetYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"5\"></a>\n",
        "# Sparse Reconstruction\n",
        "\n",
        "Now we have similar image pairs, with matched keypoint descriptors, without outliers! All that is left is to construct the scene and obtain the camera positions. We do this with pycolmap, which offers an incremental reconstruction algorithm that starts from two pairs of images and continually adds more and more images to the scene, resulting in a reconstructed scene with camera information. We can then use the camera rotation and translation as our submission!"
      ],
      "metadata": {
        "id": "XBQlO4ZvetYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
        "    mapper_options.min_model_size = 3\n",
        "    mapper_options.max_num_models = 2\n",
        "\n",
        "    maps = pycolmap.incremental_mapping(\n",
        "        database_path=database_path,\n",
        "        image_path=images_dir,\n",
        "        output_path=Path.cwd() / \"incremental_pipeline_outputs\",\n",
        "        options=mapper_options,\n",
        "    )"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.089525Z",
          "iopub.execute_input": "2024-05-15T08:53:18.089776Z",
          "iopub.status.idle": "2024-05-15T08:53:18.099444Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.089755Z",
          "shell.execute_reply": "2024-05-15T08:53:18.098662Z"
        },
        "trusted": true,
        "id": "PxEBXZSeetYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEBUG:\n",
        "    print(maps[0].summary())\n",
        "    for k, im in maps[0].images.items():\n",
        "        print(\"Rotation\", im.cam_from_world.rotation.matrix(), \"Translation:\", im.cam_from_world.translation, sep=\"\\n\")\n",
        "        print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.100502Z",
          "iopub.execute_input": "2024-05-15T08:53:18.100763Z",
          "iopub.status.idle": "2024-05-15T08:53:18.110911Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.100742Z",
          "shell.execute_reply": "2024-05-15T08:53:18.110066Z"
        },
        "trusted": true,
        "id": "7FZw8Pf_etYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running everything"
      ],
      "metadata": {
        "id": "8ILd1KrpetYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sample_submission(\n",
        "    base_path: Path,\n",
        ") -> dict[dict[str, list[Path]]]:\n",
        "    \"\"\"Construct a dict describing the test data as\n",
        "\n",
        "    {\"dataset\": {\"scene\": [<image paths>]}}\n",
        "    \"\"\"\n",
        "    data_dict = {}\n",
        "    with open(base_path / \"sample_submission.csv\", \"r\") as f:\n",
        "        for i, l in enumerate(f):\n",
        "            # Skip header\n",
        "            if i == 0:\n",
        "                print(\"header:\", l)\n",
        "\n",
        "            if l and i > 0:\n",
        "                image_path, dataset, scene, _, _ = l.strip().split(',')\n",
        "                if dataset not in data_dict:\n",
        "                    data_dict[dataset] = {}\n",
        "                if scene not in data_dict[dataset]:\n",
        "                    data_dict[dataset][scene] = []\n",
        "                data_dict[dataset][scene].append(Path(base_path / image_path))\n",
        "\n",
        "    for dataset in data_dict:\n",
        "        for scene in data_dict[dataset]:\n",
        "            print(f\"{dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
        "\n",
        "    return data_dict"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.111889Z",
          "iopub.execute_input": "2024-05-15T08:53:18.112155Z",
          "iopub.status.idle": "2024-05-15T08:53:18.122969Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.112133Z",
          "shell.execute_reply": "2024-05-15T08:53:18.122126Z"
        },
        "trusted": true,
        "id": "JZ9_e2ZQetYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_submission(\n",
        "    results: dict,\n",
        "    data_dict: dict[dict[str, list[Path]]],\n",
        "    base_path: Path,\n",
        ") -> None:\n",
        "    \"\"\"Prepares a submission file.\"\"\"\n",
        "\n",
        "    with open(\"submission.csv\", \"w\") as f:\n",
        "        f.write(\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\")\n",
        "\n",
        "        for dataset in data_dict:\n",
        "            # Only write results for datasets with images that have results\n",
        "            if dataset in results:\n",
        "                res = results[dataset]\n",
        "            else:\n",
        "                res = {}\n",
        "\n",
        "            # Same for scenes\n",
        "            for scene in data_dict[dataset]:\n",
        "                if scene in res:\n",
        "                    scene_res = res[scene]\n",
        "                else:\n",
        "                    scene_res = {\"R\":{}, \"t\":{}}\n",
        "\n",
        "                # Write the row with rotation and translation matrices\n",
        "                for image in data_dict[dataset][scene]:\n",
        "                    if image in scene_res:\n",
        "                        print(image)\n",
        "                        R = scene_res[image][\"R\"].reshape(-1)\n",
        "                        T = scene_res[image][\"t\"].reshape(-1)\n",
        "                    else:\n",
        "                        R = np.eye(3).reshape(-1)\n",
        "                        T = np.zeros((3))\n",
        "                    image_path = str(image.relative_to(base_path))\n",
        "                    f.write(f\"{image_path},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.124042Z",
          "iopub.execute_input": "2024-05-15T08:53:18.125112Z",
          "iopub.status.idle": "2024-05-15T08:53:18.137645Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.125089Z",
          "shell.execute_reply": "2024-05-15T08:53:18.136794Z"
        },
        "trusted": true,
        "id": "RHfT7OSzetYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    base_path: Path = Path(\"/kaggle/input/image-matching-challenge-2024\")\n",
        "    feature_dir: Path = Path.cwd() / \".feature_outputs\"\n",
        "\n",
        "    device: torch.device = K.utils.get_cuda_device_if_available(0)\n",
        "\n",
        "    pair_matching_args = {\n",
        "        \"model_name\": \"/kaggle/input/dinov2/pytorch/base/1\",\n",
        "        \"similarity_threshold\": 0.3,\n",
        "        \"tolerance\": 500,\n",
        "        \"min_matches\": 50,\n",
        "        \"exhaustive_if_less\": 50,\n",
        "        \"p\": 2.0,\n",
        "    }\n",
        "\n",
        "    keypoint_detection_args = {\n",
        "        \"num_features\": 4096,\n",
        "        \"resize_to\": 1024,\n",
        "    }\n",
        "\n",
        "    keypoint_distances_args = {\n",
        "        \"min_matches\": 15,\n",
        "        \"verbose\": False,\n",
        "    }\n",
        "\n",
        "    colmap_mapper_options = {\n",
        "        \"min_model_size\": 3, # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
        "        \"max_num_models\": 2,\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.13882Z",
          "iopub.execute_input": "2024-05-15T08:53:18.139566Z",
          "iopub.status.idle": "2024-05-15T08:53:18.151581Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.139543Z",
          "shell.execute_reply": "2024-05-15T08:53:18.15074Z"
        },
        "trusted": true,
        "id": "SeTaVkYtetYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_from_config(config: Config) -> None:\n",
        "    results = {}\n",
        "\n",
        "    data_dict = parse_sample_submission(config.base_path)\n",
        "    datasets = list(data_dict.keys())\n",
        "\n",
        "    for dataset in datasets:\n",
        "        if dataset not in results:\n",
        "            results[dataset] = {}\n",
        "\n",
        "        for scene in data_dict[dataset]:\n",
        "            images_dir = data_dict[dataset][scene][0].parent\n",
        "            results[dataset][scene] = {}\n",
        "            image_paths = data_dict[dataset][scene]\n",
        "            print (f\"Got {len(image_paths)} images\")\n",
        "\n",
        "            try:\n",
        "                feature_dir = config.feature_dir / f\"{dataset}_{scene}\"\n",
        "                feature_dir.mkdir(parents=True, exist_ok=True)\n",
        "                database_path = feature_dir / \"colmap.db\"\n",
        "                if database_path.exists():\n",
        "                    database_path.unlink()\n",
        "\n",
        "                # 1. Get the pairs of images that are somewhat similar\n",
        "                index_pairs = get_image_pairs(\n",
        "                    image_paths,\n",
        "                    **config.pair_matching_args,\n",
        "                    device=config.device,\n",
        "                )\n",
        "                gc.collect()\n",
        "\n",
        "                # 2. Detect keypoints of all images\n",
        "                detect_keypoints(\n",
        "                    image_paths,\n",
        "                    feature_dir,\n",
        "                    **config.keypoint_detection_args,\n",
        "                    device=device,\n",
        "                )\n",
        "                gc.collect()\n",
        "\n",
        "                # 3. Match  keypoints of pairs of similar images\n",
        "                keypoint_distances(\n",
        "                    image_paths,\n",
        "                    index_pairs,\n",
        "                    feature_dir,\n",
        "                    **config.keypoint_distances_args,\n",
        "                    device=device,\n",
        "                )\n",
        "                gc.collect()\n",
        "\n",
        "                sleep(1)\n",
        "\n",
        "                # 4.1. Import keypoint distances of matches into colmap for RANSAC\n",
        "                import_into_colmap(\n",
        "                    images_dir,\n",
        "                    feature_dir,\n",
        "                    database_path,\n",
        "                )\n",
        "\n",
        "                output_path = feature_dir / \"colmap_rec_aliked\"\n",
        "                output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # 4.2. Compute RANSAC (detect match outliers)\n",
        "                # By doing it exhaustively we guarantee we will find the best possible configuration\n",
        "                pycolmap.match_exhaustive(database_path)\n",
        "\n",
        "                mapper_options = pycolmap.IncrementalPipelineOptions(**config.colmap_mapper_options)\n",
        "\n",
        "                # 5.1 Incrementally start reconstructing the scene (sparse reconstruction)\n",
        "                # The process starts from a random pair of images and is incrementally extended by\n",
        "                # registering new images and triangulating new points.\n",
        "                maps = pycolmap.incremental_mapping(\n",
        "                    database_path=database_path,\n",
        "                    image_path=images_dir,\n",
        "                    output_path=output_path,\n",
        "                    options=mapper_options,\n",
        "                )\n",
        "\n",
        "                print(maps)\n",
        "                clear_output(wait=False)\n",
        "\n",
        "                # 5.2. Look for the best reconstruction: The incremental mapping offered by\n",
        "                # pycolmap attempts to reconstruct multiple models, we must pick the best one\n",
        "                images_registered  = 0\n",
        "                best_idx = None\n",
        "\n",
        "                print (\"Looking for the best reconstruction\")\n",
        "\n",
        "                if isinstance(maps, dict):\n",
        "                    for idx1, rec in maps.items():\n",
        "                        print(idx1, rec.summary())\n",
        "                        try:\n",
        "                            if len(rec.images) > images_registered:\n",
        "                                images_registered = len(rec.images)\n",
        "                                best_idx = idx1\n",
        "                        except Exception:\n",
        "                            continue\n",
        "\n",
        "                # Parse the reconstruction object to get the rotation matrix and translation vector\n",
        "                # obtained for each image in the reconstruction\n",
        "                if best_idx is not None:\n",
        "                    for k, im in maps[best_idx].images.items():\n",
        "                        key = config.base_path / \"test\" / scene / \"images\" / im.name\n",
        "                        results[dataset][scene][key] = {}\n",
        "                        results[dataset][scene][key][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n",
        "                        results[dataset][scene][key][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n",
        "\n",
        "                print(f\"Registered: {dataset} / {scene} -> {len(results[dataset][scene])} images\")\n",
        "                print(f\"Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
        "                create_submission(results, data_dict, config.base_path)\n",
        "                gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.154656Z",
          "iopub.execute_input": "2024-05-15T08:53:18.155147Z",
          "iopub.status.idle": "2024-05-15T08:53:18.173042Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.155096Z",
          "shell.execute_reply": "2024-05-15T08:53:18.1723Z"
        },
        "trusted": true,
        "id": "w4H0dnl3etYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_from_config(Config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:53:18.173954Z",
          "iopub.execute_input": "2024-05-15T08:53:18.17424Z",
          "iopub.status.idle": "2024-05-15T08:59:25.505791Z",
          "shell.execute_reply.started": "2024-05-15T08:53:18.174218Z",
          "shell.execute_reply": "2024-05-15T08:59:25.504844Z"
        },
        "trusted": true,
        "id": "Gojip7bxetYQ",
        "outputId": "14f3df67-fb16-4205-c525-ceedf8e52188"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Looking for the best reconstruction\n0 Reconstruction:\n\tnum_reg_images = 40\n\tnum_cameras = 40\n\tnum_points3D = 12282\n\tnum_observations = 64243\n\tmean_track_length = 5.23066\n\tmean_observations_per_image = 1606.08\n\tmean_reprojection_error = 0.913796\nRegistered: church / church -> 40 images\nTotal: church / church -> 41 images\n/kaggle/input/image-matching-challenge-2024/test/church/images/00046.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00090.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00092.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00087.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00050.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00068.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00083.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00096.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00069.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00081.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00042.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00018.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00030.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00024.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00032.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00026.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00037.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00008.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00035.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00021.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00010.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00039.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00011.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00013.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00006.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00012.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00029.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00001.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00072.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00066.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00104.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00058.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00059.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00111.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00061.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00060.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00074.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00102.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00076.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00063.png\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat submission.csv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-15T08:59:25.50712Z",
          "iopub.execute_input": "2024-05-15T08:59:25.507478Z",
          "iopub.status.idle": "2024-05-15T08:59:26.481233Z",
          "shell.execute_reply.started": "2024-05-15T08:59:25.507446Z",
          "shell.execute_reply": "2024-05-15T08:59:26.4801Z"
        },
        "trusted": true,
        "id": "owStgIeqetYQ",
        "outputId": "e62b3553-ef87-4d77-9c35-3c71e5bb317a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "image_path,dataset,scene,rotation_matrix,translation_vector\ntest/church/images/00046.png,church,church,-0.21696721375931327;-0.34027467987547655;-0.9149526601902211;0.36251374604519193;0.8421679060855337;-0.3991703920480447;0.9063713433697043;-0.41828980407744776;-0.059368575214905794,5.92333490303866;5.138910148082694;11.502384010895256\ntest/church/images/00090.png,church,church,0.9985527260440824;-0.0009035806843229544;-0.05377394212700625;-0.008632023306604147;0.9842035839927694;-0.17682984315273784;0.05308428659750399;0.17703809984784216;0.9827713720488095,0.0756176888953241;-0.010867732971685288;-0.7389864528026447\ntest/church/images/00092.png,church,church,0.9387787544044988;-0.1234343294188164;-0.32164952448226136;0.14406819979236563;0.9887155725046378;0.04105935332434388;0.31295176000036296;-0.08488511652671157;0.9459681352481816,0.25265126560114204;-0.10606084686745151;-0.5526490094356777\ntest/church/images/00087.png,church,church,0.8817982779404965;-0.16375409712047986;-0.4422854199467029;0.18321473164247434;0.9830720542996219;0.0013031363311131056;0.4345850424604498;-0.08218230789761145;0.8968734075321172,0.9751337069942448;-0.04816137906164939;-0.5518069260918107\ntest/church/images/00050.png,church,church,0.9381715099952928;0.17565890159420697;0.2982920852518034;-0.19704076349423463;0.9794545680415704;0.04293817257534263;-0.28462107327947717;-0.09905907042375031;0.9535083351560284,-2.865787385673774;0.366048907340628;0.5851360596847559\ntest/church/images/00068.png,church,church,0.5602074733600397;-0.22401482740158896;-0.7974866418290634;0.529203200727621;0.8374420886826031;0.13650904894020963;0.6372688279927097;-0.498505872791039;0.5876906802576018,5.055488798129619;0.43196414220260615;0.3700375228326736\ntest/church/images/00083.png,church,church,0.9286547185263779;-0.1458816907971593;-0.34105563482935713;0.17663752139770833;0.9823990399149074;0.06075617177490953;0.3261895151490432;-0.11666472759334845;0.9380776841724794,1.5937265898722193;-0.08963022831583893;-0.7354595955193058\ntest/church/images/00096.png,church,church,0.9502587379220246;0.09808947634137391;0.2956125600070402;-0.02489552453907764;0.9699980092312036;-0.24183480920956354;-0.31046504450705087;0.22244621084337865;0.9241910730042019,-1.0360877182609298;0.001563691729748851;-1.2520725762845681\ntest/church/images/00069.png,church,church,0.5711982313550887;-0.22083077979668664;-0.7905481308504916;0.33990355544511264;0.9403053375709861;-0.01707176415269422;0.7471265980358281;-0.25895875893644;0.6121619129589074,5.09138086573397;0.276356870079666;0.6920558493973137\ntest/church/images/00081.png,church,church,0.881979834516674;-0.12974639819759423;-0.4530755385812519;0.1513221319343623;0.988417371208248;0.01152009898463878;0.44633304145310554;-0.07872085142151188;0.8913976910777213,1.66123795466471;0.01725256739222611;-0.36472660887331326\ntest/church/images/00042.png,church,church,0.7582593848601453;0.2078384985493792;0.6179367797697866;-0.2663958722126625;0.9638599150161558;0.002702497567772924;-0.5950428089974442;-0.1666650015623818;0.7862231443519343,-4.974384647536009;2.5258710762806964;6.022649197017878\ntest/church/images/00018.png,church,church,0.9817615100759775;-0.059272661825357995;-0.18064077306486795;0.08596085177537334;0.9858815282351925;0.14369531741394226;0.16957319745581453;-0.15660256653317683;0.9729956664136965,0.3014750624561642;1.1133079837886268;2.4031832486835554\ntest/church/images/00030.png,church,church,0.9563796135244973;0.07913170344989785;0.2812049223321858;-0.15063515182459075;0.9583530382654087;0.24262832704002762;-0.2502939988683623;-0.274404131813968;0.9284693245196132,-2.8399124117938643;0.0988762598468783;0.309333205267166\ntest/church/images/00024.png,church,church,0.8134605264654475;0.21665373392679652;0.5397621063562483;-0.23180009246164976;0.9719086004818501;-0.04077241033078713;-0.5334329283211974;-0.0919501597877258;0.8408296373806552,-0.1395373930444519;0.5092810281385819;1.7136967397800293\ntest/church/images/00032.png,church,church,0.9793155738136047;0.08517487599930297;0.18353813604979624;-0.10406872509585312;0.9899373723368964;0.09588378021148743;-0.1735243710348892;-0.11300105906240189;0.9783251265850845,-1.0663220781420482;0.47058902526074403;1.2609440278726194\ntest/church/images/00026.png,church,church,0.9712994955105352;0.1416050822487733;0.19111590907692103;-0.17930701244617472;0.9638388632586902;0.197138633859077;-0.15628910809332533;-0.2257490783004751;0.9615669858818359,-2.144438534418167;0.488414215198673;0.5757219877170752\ntest/church/images/00037.png,church,church,-0.7716900763361805;0.20956900088275987;0.6004791919402879;-0.08611094320908352;0.9010291874925587;-0.4251250507158353;-0.6301423105488959;-0.37977261243473215;-0.6772690981438596,-0.7407947512253396;0.46105382739032646;3.637782543453466\ntest/church/images/00008.png,church,church,0.9998286605347704;-0.0004019955267798529;-0.01850643058084008;0.0004267381879066951;0.9999990204399124;0.0013330467863558542;0.01850587657383422;-0.0013407157834830425;0.9998278526893627,-0.7774005298411094;1.3309708531413205;4.413014333136853\ntest/church/images/00035.png,church,church,-0.44080950774852834;0.22738757319378172;0.8683212938973356;-0.3534339531838548;0.8452610296379504;-0.40077204557232377;-0.8250887337828176;-0.4835583556639618;-0.29223260949495766,-0.8496856840474553;1.224733460333957;3.457280105034736\ntest/church/images/00021.png,church,church,0.9764080399214151;0.13764195392721607;0.1663791816782327;-0.1495310666707042;0.9868661325322582;0.06112034490368101;-0.15578125585943145;-0.08455725267486905;0.9841657743200262,-1.7911094802946863;0.7030052828061331;1.5955054329002392\ntest/church/images/00010.png,church,church,0.9951018613239588;-0.01883128879490754;-0.09704467090940902;0.011886026345880006;0.9973586489406174;-0.07165506095909244;0.09813769901163238;0.07015060901851813;0.9926972771631003,-0.5142392217765277;0.9298881563151451;4.203809135507496\ntest/church/images/00039.png,church,church,0.6096361364512308;0.2558616150576742;0.7502523675889967;-0.34211620516097346;0.9387123624988112;-0.04213790049533976;-0.7150526437256708;-0.23098470605643784;0.6598073827026718,-2.0300927335958416;0.7299093349554815;1.8301870013398849\ntest/church/images/00011.png,church,church,0.9997870037762596;0.01246549485027221;0.016448663113675564;-0.012634274088413354;0.9998681868112601;0.010197260466511308;-0.016319381065107864;-0.010402905406705597;0.9998127111418172,0.2960289839365237;1.0933824200355013;3.670178999679463\ntest/church/images/00013.png,church,church,0.9949567297511333;0.021016470495126446;0.09807861077145194;-0.008984528158750886;0.9925457254258055;-0.12154119134156141;-0.09990187273867843;0.12004703622700941;0.9877288721690867,0.6099717908126084;0.5988706733702418;3.8085714785979006\ntest/church/images/00006.png,church,church,0.9732578077178069;-0.07197018415926991;0.21815025168082938;0.08769965884738924;0.994134484949573;-0.06328819536136787;-0.21231582502107557;0.08072743292170201;0.9738609099966443,0.17056171753493166;1.0037036324549249;4.679603707416873\ntest/church/images/00012.png,church,church,0.9982897484664325;0.0066620915733754575;-0.058079210072925685;-0.010400128179790195;0.9978765261067749;-0.06429833573988507;0.05752751898588193;0.06479240064191948;0.9962391928539983,0.22820276458672392;0.49433431483268664;2.445966513967651\ntest/church/images/00029.png,church,church,0.9993155056551334;-0.005750396410045592;0.03654385171752492;0.005437188210910419;0.9999476815600499;0.00866435958627769;-0.03659176330247356;-0.008459733081399207;0.9992944890144282,-0.9314300129511184;0.3785392229412974;1.5866560744245182\ntest/church/images/00001.png,church,church,0.9458310597923224;0.10551477170838332;0.3070345897182596;-0.12554977483196422;0.991011379641764;0.04619198480440785;-0.2994008356230412;-0.08223793754621593;0.9505766993022549,1.1920501239680532;1.5696615534732414;4.371666724357648\ntest/church/images/00098.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\ntest/church/images/00072.png,church,church,0.738988643274279;-0.1883051419710114;-0.6468670331829545;0.3260778863004546;0.940162581096075;0.09883083108318969;0.5895498258650138;-0.28396389667357963;0.7561716129348472,3.584618006987964;0.3948940185677593;0.1142008953024722\ntest/church/images/00066.png,church,church,0.5265278475418573;-0.2521790653999264;-0.8118954025839524;0.34026873369457367;0.9376758794362977;-0.07057714922874836;0.7790928151592915;-0.2391017860606343;0.5795211137394259,5.055848752172579;0.47126310974954816;0.9077122000446607\ntest/church/images/00104.png,church,church,0.8711478139008472;0.12816957657339403;0.47399793878958146;-0.028373059497325104;0.9768582439862513;-0.2119975015203217;-0.4902004241435703;0.17123238828298354;0.8546244867615072,-0.19205100891292537;0.4276134737185474;3.6552295386925904\ntest/church/images/00058.png,church,church,0.9992660416784077;0.027795453577269022;-0.026358882920197897;-0.03802551868818729;0.8029323337909383;-0.5948560559341493;0.004630105491361612;0.5954217665768285;0.803399951462326,0.5824770398531471;0.06606730253568709;0.6376081302939066\ntest/church/images/00059.png,church,church,-0.17230804366666286;-0.28945963855742973;-0.9415535331217073;0.558309368834018;0.7587925706487951;-0.33544669233777474;0.8115421041202533;-0.583478322111795;0.030861964708629452,6.31086716638083;2.887036754270129;4.356377141280811\ntest/church/images/00111.png,church,church,0.8803834571203848;0.189862406240646;0.43460008643035447;-0.2700698250544258;0.9539796414706703;0.13032702426804357;-0.38985543221023805;-0.23211012549219665;0.8911440016182375,-3.706665126008281;0.8830321759897295;1.09300470356801\ntest/church/images/00061.png,church,church,0.1511027141564042;0.3452228594739462;0.9262770358112095;-0.4361370242438604;0.8641853257598765;-0.25093469035788285;-0.8871034132829604;-0.3660667972350514;0.28114521888837685,-5.947429059036593;2.32500763036872;4.122782437293925\ntest/church/images/00060.png,church,church,0.9733666132480769;-0.07565373437850852;-0.21641152623775536;0.05848806989434478;0.994691667453037;-0.08466187087190441;0.2216677285804202;0.06974954604929595;0.9726245004789434,0.45291479062045453;0.4629129873085142;0.7265344856892473\ntest/church/images/00074.png,church,church,0.8520002845224259;-0.17057804123005446;-0.4949733801163687;0.14486773315752047;0.9853307215512039;-0.0902037086664689;0.5030992497147126;0.0051479138980354625;0.864213309269761,3.6329801016974925;0.21420546256828027;0.2590436022718443\ntest/church/images/00102.png,church,church,0.9267298348035362;-0.1680366370454055;-0.33605877744150825;0.1039905636897292;0.9741903648801491;-0.20034743731371843;0.3610509326237822;0.1507210057860304;0.9202855005194553,-0.47204058301395424;0.5730826045294234;2.538768684864641\ntest/church/images/00076.png,church,church,0.4470614819493821;-0.26127237799465486;-0.8554956316979302;0.29615810324588054;0.9456852276047141;-0.1340515877266005;0.8440535582763838;-0.19343266215207375;0.5001573712080398,4.8299540548823785;1.3018170075905835;3.2268455743408464\ntest/church/images/00063.png,church,church,0.8858966235923421;-0.1394650326557789;-0.44242137942691084;0.3911567016862024;0.7372793966734729;0.5508316675417011;0.249366411197778;-0.661036001975023;0.7077060103314217,0.2676044038030622;0.11761009044043606;0.028316627518734712\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to do next?\n",
        "\n",
        "Here are some ways in which you can explore potential improvements:\n",
        "\n",
        "- Using a different image embedding model to obtain the image pairs\n",
        "- Trying other approaches for keypoint extraction, such as SIFT or DISK\n",
        "- Leveraging the training data to train a better models for each dataset"
      ],
      "metadata": {
        "id": "o4cVFdtdetYR"
      }
    }
  ]
}